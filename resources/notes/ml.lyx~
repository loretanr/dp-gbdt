#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%% use full paper
\usepackage[margin=0.5cm]{geometry}

%% multicols
\usepackage{multicol}

%% no automated date after title 
\date{}

%% compact spacing
\usepackage[compact]{titlesec}
\usepackage{enumitem}
%% itemize compact
\setlist[itemize,1]{leftmargin=\dimexpr 26pt-.15in}
\setlist[enumerate,1]{leftmargin=\dimexpr 26pt-.15in}
\setlist{nolistsep}

%% table formatting
\usepackage{graphicx} 

%% colors
\usepackage{xcolor}

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}

% highlight
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\newcommand{\hilightt}[1]{\colorbox{orange}{#1}}

% titlespace
\usepackage{titling}

% no ugly indent
\setlength\parindent{0pt}

% compact TOC
\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}
\setcounter{tocdepth}{2}

% argmin/max
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

% strikethrough
\usepackage{ulem}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation landscape
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "frame=trbl,backgroundcolor={\color{lightgray}},flexiblecolumns=true,basicstyle={\small\ttfamily},breaklines=true,keywordstyle={\color{blue}\bfseries},language=Java,sensitive=true,emph={[1]{critical_section}},emphstyle={[1]\color{red}},emph={[2]{atomic,Condition}},emphstyle={[2]\color{blue}},rulesepcolor={\color{gray}},emph={[3]{acquire(mutex),release(mutex),signal(mutex)}},emphstyle={[3]\color{magenta}},showstringspaces=false,stringstyle={\color{purple}},commentstyle={\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small},morecomment={[l][\color{Blue}]{...}},tabsize=4,lineskip={-1.5pt}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength{
\backslash
droptitle}{-1.1cm}
\end_layout

\begin_layout Plain Layout


\backslash
titlespacing{
\backslash
section}{0pt}{*1.7}{*1.3} 
\end_layout

\begin_layout Plain Layout


\backslash
titlespacing{
\backslash
subsection}{0pt}{*2.5}{*0.9} 
\end_layout

\begin_layout Plain Layout


\backslash
titlespacing{
\backslash
subsubsection}{0pt}{*1.7}{*0.5}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength{
\backslash
columnseprule}{0.2pt}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{multicols*}{3}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{small}
\end_layout

\end_inset


\end_layout

\begin_layout Title
ML Notes
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-2.3cm}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Decision Trees
\end_layout

\begin_layout Paragraph
Sources
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset CommandInset href
LatexCommand href
target "https://gdcoder.com/decision-tree-regressor-explained-in-depth/"

\end_inset


\end_layout

\begin_layout Standard
(
\begin_inset CommandInset href
LatexCommand href
target "https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html"

\end_inset

)
\end_layout

\begin_layout Paragraph
Keywords
\end_layout

\begin_layout Itemize
supervised learning
\end_layout

\begin_layout Itemize
DT can be used for classification and regression (our use case)
\end_layout

\begin_layout Itemize
Decision trees are predictive models that use a set of binary rules to calculate
 a target value.
\end_layout

\begin_layout Paragraph
How it works
\end_layout

\begin_layout Standard
A decision tree is arriving at an estimate by asking a series of questions
 to the data, each question narrowing our possible values until the model
 get confident enough to make a single prediction.
 The order of the questions (True/false form) as well as their content are
 being determined by the model.
\end_layout

\begin_layout Paragraph
Splitting
\end_layout

\begin_layout Standard
deciding the splits affects the trees accuracy.
\end_layout

\begin_layout Itemize
regression DTs normally use mean squared error (MSE) to decide to split
 a node in two or more sub-nodes.
\end_layout

\begin_layout Itemize
we need to pick an attribute and a value to split on.
 
\begin_inset Formula $\rightarrow$
\end_inset

 try out all combinations.
 Find the best one by taking the weighted average of the two new nodes.
\end_layout

\begin_layout Itemize
Do that until you hit 
\family typewriter
max_depth
\family default
 or when you're left with only 1 element.
\end_layout

\begin_layout Itemize
It is never necessary to do more than one split at a level because you can
 just split them again.
\end_layout

\begin_layout Itemize
Instead of using the average (MSE) we could also use the median or...
 or even run a linear regression model (
\begin_inset Quotes eld
\end_inset

fitting a line into points
\begin_inset Quotes erd
\end_inset

) to make a decision.
\end_layout

\begin_layout Paragraph
Prediction
\end_layout

\begin_layout Itemize
Traverse tree until you end up in leaf.
 The prediction is the average of the value of the 
\bar under
dependent variable
\bar default
 in that leaf node.
\end_layout

\begin_layout Paragraph
Pros
\end_layout

\begin_layout Itemize
If there is a high non-linearity & complex relationship between dependent
 & independent variables, a tree model will outperform a classical regression
 method
\end_layout

\begin_layout Itemize
It's kinda robust to outliers
\end_layout

\begin_layout Itemize
easy to explain
\end_layout

\begin_layout Paragraph
Cons
\end_layout

\begin_layout Itemize
DTs are prone to overfitting.
 That's why they are rarely used and instead other tree based models are
 preferred like Random Forest and XGBoost.
\end_layout

\begin_layout Itemize
Mostly for classifiction.
 For regression only if the range of the target variable is inside the range
 of values seen in training.
\end_layout

\begin_layout Itemize
Unstable, small changes can change entire tree (variance)
\begin_inset Formula $\rightarrow$
\end_inset

 can be improved by bagging or boosting
\end_layout

\begin_layout Paragraph
Avoiding Overfitting
\end_layout

\begin_layout Enumerate
Setting constraints on tree size (hyperparameters)
\end_layout

\begin_layout Standard
e.g.
 min-sample-for-node-split, min-leaf-size, max-depth, max-features to consider
 for split.
 
\bar under
sklearn
\bar default
 lets you define many of these.
\end_layout

\begin_layout Enumerate
Pruning
\end_layout

\begin_layout Enumerate
Random Forest
\end_layout

\begin_layout Standard

\bar under
Random Forest
\bar default
 is an example of 
\bar under
ensemble learning
\bar default
, in which we combine multiple machine learning algorithms to obtain better
 predictive performance.
 RF can be fast to train, but quite slow to create predictions.
 Due to the fact that it has to run predictions on each individual tree
 and then average to create the final prediction.
\end_layout

\begin_layout Subsubsection
DT vs (Logit) Regression
\end_layout

\begin_layout Itemize
Bsp: chance of buying BWM with regard to salary and age.
\end_layout

\begin_layout Standard
Converting variables into buckets (DT) might make the analysis simpler,
 but it makes the model lose some predictive power because of its indifference
 for data points lying in the same bucket.
\end_layout

\begin_layout Standard
Logistic regression is a statistical model that uses a logistic function
 to model a binary dependent variable.
 Sth.
 like this:
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename lyx_images/2021-04-30-102246_690x535_scrot.png
	lyxscale 20
	scale 10

\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset


\begin_inset space \thinspace{}
\end_inset

 
\begin_inset Graphics
	filename lyx_images/2021-04-30-102445_835x318_scrot.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now assuming we have an exceptionally high buy rate for people between 100k
 and 200k salary that are younger than 35.
 Both techniques cannot handle this well.
 DT will fail because it splits one-dimentional.
 Regression fails as well.
\end_layout

\begin_layout Paragraph
Trick: Combine the upsides of both methods
\end_layout

\begin_layout Standard
Two possible ways:
\end_layout

\begin_layout Enumerate
Introduce a new covariant variable:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $Z=\begin{cases}
1 & \text{if 100k<salary<200k \,\&\&\, Age\ensuremath{\geq35}}\\
0 & \text{o/w}
\end{cases}$
\end_inset


\end_layout

\begin_layout Enumerate
Make two alternative models and add the functions in the regression formula
 as following 
\begin_inset Formula $H(x)=(1-z)*f_{x}+z*g_{x}$
\end_inset

, where z is the split in 1.
\end_layout

\begin_layout Standard
Note: this technique fails when overall covariance between two terms is
 high.
 This is because we will have to create too many buckets and, therefore,
 too many variables to be introduced in the regression model.
\end_layout

\begin_layout Subsubsection
ID3
\end_layout

\begin_layout Standard
The ID3 algorithm builds decision trees using a top-down greedy search approach
 through the space of possible branches with no backtracking (until the
 branches have 0 entropy).
\end_layout

\begin_layout Enumerate
It begins with the original set S as the root node.
\end_layout

\begin_layout Enumerate
On each iteration of the algorithm, it iterates through the unused attributes
 of the set S and calculates 
\bar under
Entropy(H)
\bar default
 and 
\bar under
Information gain(IG)
\bar default
 of this attribute.
\end_layout

\begin_layout Enumerate
It then selects the attribute which has the smallest Entropy Largest Information
 gain.
\end_layout

\begin_layout Enumerate
The set S is then split by the selected attribute and the algorithm continues
 to recur on each subset, considering only attributes never selected before.
\end_layout

\begin_layout Paragraph
Attribute Selection Method
\end_layout

\begin_layout Standard
can be Entropy, Information Gain, Gini Index, Gain Ratio, Reduction in Variance,
 Chi-Square, etc.
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Section
Gradient Boosted DT
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
target "https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Similar to AdaBoost in the sense that both use ensemble of trees to predict
 a target label.
 However, unlike AdaBoost, GB trees have depth larger than 1.
 In practice, you’ll typically see GB being used with a maximum number of
 
\bar under
leaves
\bar default
 of between 8 and 32.
 In GB all trees are equally important.
\end_layout

\begin_layout Enumerate
When tackling regression problems, we start with a leaf that is the initial
 guess / 
\bar under
average value
\bar default
 of the variable we want to predict.
\end_layout

\begin_layout Enumerate
Calculate residuals (actual value vs predicted)
\end_layout

\begin_layout Enumerate
Build a tree with the goal of predicting the residuals.
 Take average if there's more residuals than leafs.
\end_layout

\begin_layout Enumerate
Pass samples through the tree
\end_layout

\begin_layout Enumerate
Prediction = average/init price + learning rate * residual predicted by
 the tree
\end_layout

\begin_layout Enumerate
Compute new residuals by subtracting actual values from predicted value
 in 5.
\end_layout

\begin_layout Enumerate
repeat 3 - 6 until the hyperparam 
\family typewriter
numtrees
\family default
 is reached
\end_layout

\begin_layout Enumerate
make the ensemble (initial mean + l-rate * residual
\begin_inset Formula $_{i}$
\end_inset

, 
\begin_inset Formula $\forall$
\end_inset

tree 
\begin_inset Formula $i$
\end_inset

)
\end_layout

\begin_layout Standard
to prevent overfitting, we introduce a hyperparameter called 
\bar under
learning rate
\bar default
.
 When we make a prediction, each residual is multiplied by the learning
 rate.
 This forces us to use more decision trees, each taking a 
\bar under
small step
\bar default
 towards the final solution.
\end_layout

\begin_layout Paragraph
Math
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "https://www.youtube.com/watch?v=2xudPOBz-vs"
target "https://www.youtube.com/watch?v=2xudPOBz-vs"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
Need differentiable 
\bar under
loss function
\end_layout

\begin_layout Standard
Most common for regression: 
\begin_inset Formula $L(y_{i},F(x))=\frac{1}{2}(\text{observed}-\text{predicted})^{2}$
\end_inset

.
 MSE would be the same but 
\begin_inset Formula $\frac{1}{n}$
\end_inset

.
 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 would not be necessary to determine the best fit, it's only there to make
 derivatives easier.
 
\begin_inset Formula $\frac{d}{d\,\text{predicted}}\frac{1}{2}(\text{observed}-\text{predicted})^{2}=\text{-}(\text{observed}-\text{predicted})$
\end_inset

.
\end_layout

\begin_layout Enumerate

\bar under
Initialize model
\bar default
 with const value 
\begin_inset Formula $F_{0}(x)=\underset{\gamma}{\argmin}\sum_{i=1}^{n}L(y_{i},\gamma)$
\end_inset


\begin_inset Newline newline
\end_inset

Where 
\begin_inset Formula $\gamma$
\end_inset

 is the predicted value we want to find (done by setting the sum of derivatives
 =0).
\end_layout

\begin_layout Standard
Now the first iteration (
\begin_inset Formula $m=1)$
\end_inset


\end_layout

\begin_layout Enumerate
Compute 
\begin_inset Formula $r_{im}=-\left[\frac{\partial L(y_{i},F(x_{i}))}{\partial F(x_{i})}\right]_{F(x)=F_{m-1}(x)}\text{ for }i=1\dots n$
\end_inset


\begin_inset Newline newline
\end_inset

This is just the derivative of the loss function with respect to the predicted
 value.
 Which is 
\begin_inset Formula $(\text{observed}-\text{predicted})$
\end_inset

 for the case where we have 
\begin_inset Formula $\frac{1}{2}$
\end_inset

 (we have the extra minus that cancels).
 So there it's equal to the actual 
\bar under
residual
\bar default
.
 But since one can take any loss function, we call it 
\bar under
pseudo residual
\bar default
.
 We the plug in the 
\begin_inset Formula $F_{m-1}$
\end_inset

 which is the last prediction.
 We do this for 
\begin_inset Formula $r_{im}$
\end_inset

, where 
\begin_inset Formula $i$
\end_inset

 is the sample number and 
\begin_inset Formula $m$
\end_inset

 is the current tree.
\end_layout

\begin_layout Enumerate
Fit the regression tree to the 
\begin_inset Formula $r_{im}$
\end_inset

values and 
\bar under
create terminal regions
\bar default
 
\begin_inset Formula $R_{jm}$
\end_inset

 for 
\begin_inset Formula $j=1\dots J_{m}$
\end_inset

.
 They may contain more than one 
\begin_inset Formula $r_{im}$
\end_inset

 now.
\end_layout

\begin_layout Enumerate
For these regions, for 
\begin_inset Formula $j=1..J_{m}$
\end_inset

 compute
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\gamma_{jm}=\underset{\gamma}{\argmin}\sum_{x_{i}\in R_{ij}}L(y_{i},F_{m-1}(x_{i})+\gamma)$
\end_inset


\begin_inset Newline newline
\end_inset

This is the step where we create 1 value from the samples that ended up
 in the same leaf.
 Need the sum when more than one sample end up in one leaf.
 In we use the 
\begin_inset Formula $\frac{1}{2}()^{2}$
\end_inset

 loss function, this nicely simplifies to the 
\bar under
average
\bar default
 of the 
\begin_inset Formula $r_{im}$
\end_inset

's in their corresp region.
\end_layout

\begin_layout Enumerate
Update 
\begin_inset Formula $F_{m}(x)=F_{m-1}(x)+\nu\sum_{j=1}^{J_{m}}\gamma_{jm}I(x\in R_{jm})$
\end_inset


\begin_inset Newline newline
\end_inset

where 
\begin_inset Formula $\nu$
\end_inset

 is the learning rate.
 
\begin_inset Quotes eld
\end_inset

Add up the output values (
\begin_inset Formula $\gamma_{jm}$
\end_inset

's for all the leaves 
\begin_inset Formula $R_{jm}$
\end_inset

 that contain a sample 
\begin_inset Formula $x$
\end_inset

.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsection
Differential Privacy
\end_layout

\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
target "https://www.youtube.com/watch?v=MOcTGM_UteM"

\end_inset


\end_layout

\begin_layout Itemize
Differential Privacy protects the data 
\bar under
before
\bar default
 it enters the model.
\end_layout

\begin_layout Itemize
Example fitness data, and Eve wants to find out whether Bob is in the data.
\end_layout

\begin_layout Standard
We want to create a function 
\begin_inset Formula $f$
\end_inset

 that wenn applied to the whole dataset, should be as similar as possible
 to the function applied to the dataset w/o Bob: 
\begin_inset Formula $f(D_{n})=f(D_{n-1})$
\end_inset

.
 In terms of probability distributions: 
\begin_inset Formula $\frac{P(f(D_{n}))}{P(f(D_{n-1}))}=e^{\epsilon}$
\end_inset

.
 
\begin_inset Formula $\epsilon=0$
\end_inset

 would achieve peak privacy.
 
\begin_inset Formula $\Rightarrow$
\end_inset

 Add random noise to the data.
 
\begin_inset Formula $f(D_{n})$
\end_inset

=D
\begin_inset Formula $_{n}+\text{noise}$
\end_inset

.
 But the more noise you add the less accurate the data becomes.
 That means 
\begin_inset Formula $\epsilon=0$
\end_inset

 would add so much noise that the data would not be useful anymore.
 Therefore it works best for last datasets, as good privacy in small ones
 makes them kinda unusable.
\end_layout

\begin_layout Section
GBDT paper
\end_layout

\begin_layout Itemize
Goal: improve model accuracy of GBDT while preserving the strong guarantee
 of differential privacy.
\end_layout

\begin_layout Itemize

\bar under
Sensitivity
\bar default
 and 
\bar under
privacy budget
\bar default
 are two key design aspects for the effectiveness of differential private
 models.
\end_layout

\begin_layout Paragraph
Algorithm
\end_layout

\begin_layout Standard
GBDT minimizes the following objective for the 
\begin_inset Formula $t^{\text{th}}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $O(x)^{(t)}=\sum_{i}L(y_{i},\gamma_{i})+\sum_{k}\Omega(f_{k})$
\end_inset

, where 
\begin_inset Formula $\Omega$
\end_inset

 is the regularization function (that penalizes the complexity of the 
\begin_inset Formula $t^{\text{th}}$
\end_inset

 tree)
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $L$
\end_inset

 is replaced with an approximation.
\end_layout

\begin_layout Itemize
Then the tree is built, splits are decided by this simplified formula of
 the 
\bar under
gain
\bar default
:
\end_layout

\begin_layout Standard
\begin_inset Formula $G(I_{L},I_{R})=\frac{(\sum_{i\in I_{L}}g_{i})^{2}}{|I_{L}|+\lambda}+\frac{(\sum_{i\in I_{R}}g_{i})^{2}}{|I_{R}|+\lambda}$
\end_inset


\end_layout

\begin_layout Itemize
Optimal leaf values are set by
\end_layout

\begin_layout Standard
\begin_inset Formula $V(I)=-\eta\frac{\sum_{i\in I}g_{i}}{|I|+\lambda}$
\end_inset

, 
\begin_inset Formula $\lambda$
\end_inset

 is the regularization parameter.
\end_layout

\begin_layout Paragraph
Differential Privacy
\end_layout

\begin_layout Standard

\bar under
Privacy budget allocation
\bar default
: either 
\bar under
sequential
\bar default
, but with large #trees the budget for each tree is very small.
 
\bar under
parallel composition
\bar default
 means giving disjoint inputs to each tree.
 Problem with large #trees they can get too few input to do meaningful work.
\end_layout

\begin_layout Itemize
Laplace mechanism works to hide numbers
\end_layout

\begin_layout Itemize
Exponential mechanism works to hide answers to non-numeric questions like
 
\begin_inset Quotes eld
\end_inset

What's the most common eye color?
\begin_inset Quotes erd
\end_inset

.
 Or 
\begin_inset Quotes eld
\end_inset

Which price would bring the most money from a set of buyers?
\begin_inset Quotes erd
\end_inset

.
 i.e.
 
\begin_inset Formula $R=\{\text{red,blue,green,red}\}$
\end_inset

 or 
\begin_inset Formula $R=\text{\{1\$, 2\$, 2.5\$, 2\$\}}$
\end_inset


\begin_inset Formula $\Rightarrow$
\end_inset

 We need to perturb the answer with the same distribution as the real answers.
\end_layout

\begin_layout Section
Theos Thesis
\end_layout

\begin_layout Itemize
We have a convex loss function
\end_layout

\begin_layout Itemize
Most notes handwritten in the thesis!
\end_layout

\begin_layout Paragraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hilightt{
\end_layout

\end_inset

Questions
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
I think I understand the individual concepts, but I'm having trouble putting
 them together and into the insurance context.
\end_layout

\begin_layout Itemize
The output of the whole process is a model (an ensemble of trees).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
sout{
\end_layout

\end_inset

This would then allow the insurance company to run past & future customer
 data through it and get an idea of where they stand in comparison with
 others.
 They will know a company has bad security but not in what way.
 Right? Couldn't that lead to discrimination if a company is worse than
 the rest?
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
I guess, the insurance won't ever have the questionnairs, they're only sent
 to the enclave.
 And probably encrypted such that only the one legit enclave can decrypt
 it.
\end_layout

\begin_layout Itemize
How do the output trees look?
\end_layout

\begin_layout Standard
In the algorithm: 
\emph on
Output: An ensemble of trained differentially private decision trees.
 
\emph default
The node's split attributes should be visible, however the split value is
 mangled (exp mechanism, meaning it could actually stand for another value
 in the set of possible values).
 And we also see leafs containing numbers that were mangled (laplace mechanism,
 so they are all 
\begin_inset Quotes eld
\end_inset

off
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Itemize
How can the insurance now learn from the model?
\end_layout

\begin_layout Itemize
The 2-node algorithm internal nodes get half the budget compared to non-2-node
 algorithm.
\end_layout

\begin_layout Standard
I guess because now two internal nodes depend on the same input, which violates
 the disjointness requirement from the parallel composition.
\end_layout

\begin_layout Itemize
In the algorithm (2 & 3), what does 
\begin_inset Quotes eld
\end_inset

update gradients of all instances on loss 
\begin_inset Formula $l$
\end_inset


\begin_inset Quotes erd
\end_inset

 exactly mean?
\end_layout

\begin_layout Standard
Does it mean: take the leaf values of the last tree, plug them into the
 loss function and calculate the derivative.
\end_layout

\begin_layout Itemize
Why do we even need differential privacy inside the enclave? What would
 be a good attack if the output trees are not DP?
\end_layout

\begin_layout Standard
One could observe effects of e.g.
 adding / leaving away one company on the output trees.
 One could pretty accurtely follow in which leafs it ended up (where it
 influenced the average) and thus which internal node path it took.
\end_layout

\begin_layout Itemize
Let's say we need 500 companies' data to get a meaningful model.
 Until that point we can't really start.
 We would need to store the 500 questionnaires somewhere.
 
\begin_inset Formula $\rightarrow$
\end_inset

 do we store enclave-encrypted questionnaires?
\end_layout

\begin_layout Itemize
Could a malicious insurance e.g.
 give 400 distinct legit questionnaires from + 100 identical ones from one
 company to the enclave? And would differential privacy
\end_layout

\begin_layout Section
Differential Privacy
\end_layout

\begin_layout Paragraph
Epsilon
\end_layout

\begin_layout Standard
\begin_inset CommandInset href
LatexCommand href
name "https://aircloak.com/explaining-differential-privacy/"
target "https://aircloak.com/explaining-differential-privacy/"
literal "false"

\end_inset

 The problem is that when you use truly random noise to anonymize your data,
 every time you query the same data, you reduce the level of anonymization.
 This is because you are able to use the aggregate results to reconstruct
 the original data by filtering out the noise through 
\bar under
averaging
\bar default
.
 The value epsilon is then used to determine how strict the privacy is.
 
\bar under
The smaller the value, the better the privacy but the worse the accuracy
\bar default
 of any results from analysing the data.
 That also means, the smaller the value of epsilon, the fewer times you
 can access the data (effectively epsilon is proportional to your privacy
 budget) because otherwise you would be able to reconstruct the noise and
 ultimately de-anonymize the data.
 The trick to all this is that you can exactly define how much of your privacy
 budget you can use until the data is not considered as anonymous anymore.
\end_layout

\begin_layout Itemize
the inventors of the concept suggested to keep the epsilon between 0.1 and
 1
\end_layout

\begin_layout Itemize
value of 
\begin_inset Formula $\epsilon$
\end_inset

 that would generally be regarded as strongly anonymous (i.e.
 
\begin_inset Formula $\epsilon<1$
\end_inset

) allow for only a small number of queries, maybe a few 10s
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{small}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{multicols*}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
