<?xml version="1.0" ?>
<cherrytree>
	<node custom_icon_id="0" foreground="" is_bold="True" name="Meeting notes" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619168425.57" ts_lastsave="1625144466.74" unique_id="1">
		<rich_text>Thu 3pm
Link:  </rich_text>
		<rich_text foreground="#3d3c40"> </rich_text>
		<rich_text link="webs https://ethz.zoom.us/j/4333973638">https://ethz.zoom.us/j/4333973638</rich_text>
		<rich_text>



</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="04_22_first" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619168455.86" ts_lastsave="1619595361.65" unique_id="2">
			<rich_text scale="h2">Discussion points</rich_text>
			<rich_text>

More details on the tasks in the thesis description:

• Task 0
   ◇ ~2 weeks
   ◇ Also hello world enclave in SGX dev environment, with i/o and r/w from disk

• Task 3
   ◇ depends on the results from 1 and 2

• Task 4
   ◇ Running the algorithm multiple times on the same input, can weaken the randomness/privacy guarantees of the privacy-preserving part. we need to avoid that. Kari and Esfandiari have ideas.

• Task 5
   ◇ we'll see how much time we have for this

• Task ...
   ◇ Where “real research” resp new things would happen
   ◇ This could very well be the hardest task if we get here

• Task n
   ◇ 1 month for writing


</rich_text>
			<rich_text scale="h2">Organization</rich_text>
			<rich_text>

• Weekly thursday meetings 3PM
   ◇ maybe daily updates to mattermost?


</rich_text>
			<rich_text scale="h2">Next steps</rich_text>
			<rich_text>

• Read about the ML algorithm
• Fully read the papers
• Hello world enclave as described above.
• prepare eth paperwork for thesis start
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="04_29" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619595361.65" ts_lastsave="1620053169.37" unique_id="3">
			<rich_text scale="h2">What I did the past week</rich_text>
			<rich_text>

• Reading “Intel SGX explained” paper (120 pages, from MIT)
   ◇ gone through background on computer architecture (CPU and memory)

• Read Theos thesis (except synthetic data generation)
   ◇ understand the idea of  Decision Trees and Gradient Boosting, Differential privacy
      ▪ how exactly the formulas are derived, not clear

• Working/backup private github repo
   ◇ because no permission for ethz gitlab
   ◇ add and share link with you in mettermost?
      ▪ would need github names
   ◇ Time log excel
   ◇ </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">write in mattermost whenever I finished a task</rich_text>
			<rich_text>
      ▪ then you can add something if you want

• Read side channel papers
   ◇ finished 2/6 (oldest ones)

• can run enclaves in my OS
   ◇ not yet created my own enclave
   ◇ but checked out and ran the SampleEnclave that installation comes with

----------------------------- still TODO ----------------------------------

• read the GBDT paper
• run theos algorithm to understand it better
   ◇ compare code and thesis
   ◇ understand what's new compared to the GBDT paper
• need to read the rest of the side-channel papers
• play around with the hello world enclave

-------------------------------------------------------------------------------

</rich_text>
			<rich_text scale="h2">Questions</rich_text>
			<rich_text>   </rich_text>
			<rich_text background="#ffffffff0000" foreground="#1a1a1a1a1a1a">TODO</rich_text>
			<rich_text>

• Looking at the the python code.... when generating a cpp algorithm
   ◇ math, ML and other libraries?
      ▪ just use whatever cpp libraries I can find?
      ▪ or what's the strategy here
      ▪ other recommandations from you guys, maybe you've already done it?

==========================================

</rich_text>
			<rich_text scale="h2">Discussion Points</rich_text>
			<rich_text>

• Order for reading papers:
   ◇ </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1ht7h--tp0ivNH9Z8S1tpoRoyzN_Q7DVU3sHaoRdDktw/edit?ts=608aacbd#">https://docs.google.com/document/d/1ht7h--tp0ivNH9Z8S1tpoRoyzN_Q7DVU3sHaoRdDktw/edit?ts=608aacbd#</rich_text>
			<rich_text>
   ◇ skip rollback paper for now

• Take notes when reading!

• ML: do Tutorials

• Theo could explain his code

• Can officially start mid May

</rich_text>
			<rich_text scale="h3">Next steps</rich_text>
			<rich_text>

• Read read read and make notes
• submit thesis proposal</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_06" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1620053169.39" ts_lastsave="1620308160.81" unique_id="4">
			<rich_text scale="h2">Questions</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">DP-GBDT proposes 2 level structure Ensemble of Ensembles.</rich_text>
			<rich_text>
   ◇ The first step splits the input data trees are created using parallel composition
   ◇ 2nd step would create multiple such ensembles and do sequential composition
      ▪ this is left away in Theos thesis, right? Was there a reason (besides time maybe)
      ▪ if an enclave would do multiple such 2nd steps that would be similar to the adversary doing rollback attack
</rich_text>
			<rich_text foreground="#ffffa5a50000">• It was sufficient to do the inner level
• and by doing the outer you would gain accurracy and lose privacy</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">How can the insurance now learn from the model?</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffa5a50000">• The trees are not useless, need to check evaluation/appendix of thesis
• especially the inner nodes have a good probability to be the most important ones on top etc.</rich_text>
			<rich_text>

Let's say we need 500 companies' data for a meaningful model. Until we collected that many, the questionnaires are not useful. So we would need to store them somewhere. (encrypted etc probably).
   ◇ And when we have 500 we feed it to the enclave
   ◇ </rich_text>
			<rich_text foreground="#9090eeee9090">What if we feed 400 distinct, and then 100x the same company, or maybe some other clever pattern?</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffa5a50000">• we need to define those things, and specify such rules in the enclave code (attestation)
• Or there may be some approaches where the data deletes itself after 1 run or something
• we kind of need to find out this kind of things</rich_text>
			<rich_text>

-------------------------------------------------------------
</rich_text>
			<rich_text scale="h3">3 options for coding</rich_text>
			<rich_text>

• \exists way to run python code in enclave -&gt; bad, much code, how to secure
• use C DPDT implementation, start from there
   ◇ </rich_text>
			<rich_text link="webs https://github.com/yarny/gbdt">https://github.com/yarny/gbdt</rich_text>
			<rich_text>
   ◇ however this code is (highly) optimized
• </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">do it from scratch</rich_text>
			<rich_text>
   ◇ can use libraries as much as we want for non-secret dependant parts
   ◇ but do the secret dependant parts on my own
      ▪ then we know them exactly

------------- next steps -------------------
• finish reading
• Tell Kari to setup a meeting with Theo as soon as I'm ready
• get to know Theos code

</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_20" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621517715.69" ts_lastsave="1621517939.67" unique_id="6">
			<rich_text>• we'll try and find secret dependant accesses by hand first!
   ◇ because tools can be messy!
   ◇ esfanidar sent a paper for such a tool </rich_text>
			<rich_text link="webs https://hal.inria.fr/hal-01658653/document">https://hal.inria.fr/hal-01658653/document</rich_text>
			<rich_text>

• TODO create a shared google docs with the 2 algorithms from the DPGDBDT paper
   ◇ (</rich_text>
			<rich_text link="webs https://ojs.aaai.org//index.php/AAAI/article/view/5422">https://ojs.aaai.org//index.php/AAAI/article/view/5422</rich_text>
			<rich_text>)
   ◇ need to be able to annotate each part with our ideas whether it is potentially risky
   ◇ and need space to insert solutions like “use list, loop through all nodes etc”</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_28" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622210274.9" ts_lastsave="1622213346.65" unique_id="10">
			<rich_text>
• High level analysis of the algorithm side channel was ok.
    But at some point we need a more detailed overview of the problem and how my implementation solves it.

• Had difficulties to explain the The [0,1] probabilities addition thingy. 
        The thing that Theo does to implement the exponential mechanism.
        is named something like CTF (probability distribution) with increasing balkens something.
        
        
        
</rich_text>
			<rich_text scale="h2">Q &amp; A</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">do you see any difference in this approach vs creating all 50 sets at once</rich_text>
			<rich_text>
    “unused samples are not put back”
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar sees privacy dangers in this approach</rich_text>
			<rich_text>
        </rich_text>
			<rich_text foreground="#ffffa5a50000">if there's a tree whose ability to improve the model (discard tree or not) depends on 1 point</rich_text>
			<rich_text>
            then someone might learn something somehow. Esfandiar thinks so.
            maybe an adversary could recognize whether a sample “put back” resp. used again

</rich_text>
			<rich_text foreground="#9090eeee9090">is it normal for GBDT that you end up using less tree than specified?</rich_text>
			<rich_text>
   •  when tested, out of 50 possible trees it ended up being ~25
   •  so half of the training data was not &quot;used&quot; 
   •  but we still paid privacy budget for it
</rich_text>
			<rich_text foreground="#ffffa5a50000">   • It's not normal, normal GBDT improve with every tree.
      ◇  but it makes sense because of DP! Randomization can create useless trees</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">Do you think that leaks information if we don't keep trees that are not used for prediction later?
    </rich_text>
			<rich_text foreground="#ffffa5a50000">YES, must keep all trees! (Esfandiar)</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#9090eeee9090">start with bfs / dfs / 2-nodes ?   Need all 3 in final implementation?</rich_text>
			<rich_text>
   •  2-nodes was best
   </rich_text>
			<rich_text foreground="#ffffa5a50000">• would be nice to have all options</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">which dataset am I working with? synthetic I guess?</rich_text>
			<rich_text>
       </rich_text>
			<rich_text foreground="#ffffa5a50000">• would be nice to have all abalones and datasets!</rich_text>
			<rich_text>
       
    
</rich_text>
			<rich_text scale="h2">TODO</rich_text>
			<rich_text> next steps:
   ◇ we need to check with higher privacy budget. Then less trees should get discarded.

   ◇ </rich_text>
			<rich_text weight="heavy">Remove the “put back ” strategy in python code and see whether it works as good as Theo’s code does.</rich_text>
			<rich_text>
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_03" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622727420.89" ts_lastsave="1623056086.55" unique_id="13">
			<rich_text> discussing and Q&amp;A also here
 
    </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1GaqAwNijzrrf_228VdCj4WU4N1dySOratf2DJ6VN3XI/edit#">https://docs.google.com/document/d/1GaqAwNijzrrf_228VdCj4WU4N1dySOratf2DJ6VN3XI/edit#</rich_text>
			<rich_text>
    
    
    
Q &amp; A

   •  The scaling to [-1,1] in Theos code is necessary for the proof.
      ◇ If those values would be larger we would need more noise
      ◇  the 3 in delta G sensitivity calculation (in code) is because of the scaling!


TODO
   ◇ create clean graphs
   ◇ especially one that uses alltrees and no rejection for pb 0.5 to 4
   ◇ debugger how many instances where (abalone + maybe some larger dataset?)
   ◇ put __pycache__'s in gitignore if not already.
   ◇ done </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1CEYstt6WXV2DO6hBD7ENk2zDl_5xUWXYuJB7Cd5PjUw/edit">https://docs.google.com/document/d/1CEYstt6WXV2DO6hBD7ENk2zDl_5xUWXYuJB7Cd5PjUw/edit</rich_text>
			<rich_text>

TODO 
    check out Theos meeting slides.</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_10" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1623331472.64" ts_lastsave="1623332681.42" unique_id="14">
			<rich_text foreground="#9090eeee9090">How does the mighty “proof” of Moritz look?</rich_text>
			<rich_text>
•  got the DPGBDT proof from esfandiar 
   ◇ I don't understand anything
    
</rich_text>
			<rich_text foreground="#9090eeee9090">do you have an idea of how I could create a better overview of algorithm and implementation to reason about side channel stuff?</rich_text>
			<rich_text>
   •  need some form of more detailed pseudocode (with more details)

</rich_text>
			<rich_text foreground="#9090eeee9090">Only y is scaled right? because we only will add noise to predictions. Right?
Or do we need to scale X before training? Or does it not make a difference?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar says only the labels (y)</rich_text>
			<rich_text>
    o/w read the base paper...
    
    
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_17" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1623939008.65" ts_lastsave="1623939129.56" unique_id="15">
			<rich_text>• need to indicate after each pseudocode, what exactly it could leak in its current form (even algorithm parameters)

• special consideration to all branches like the ones on top of find_best_split

• special consideration required for stuff like “continue;”</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="07_01" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1625144466.79" ts_lastsave="1625154146.07" unique_id="20">
			<rich_text>
    SLIDES IN GOOGLE SLIDES
    
    
   •  need DPBoost like Log plots for comparison !!!!!!!!!!!!! I was reading them wrong!!!

otherwise go on as roadmap


And for the hardened version will need big comments with formulas etc to make it understandable.

Esfandiar said BFS is better for small datasets than DFS -&gt; might be important to do it doch no</rich_text>
		</node>
	</node>
	<node custom_icon_id="0" foreground="#ff0000" is_bold="True" name="Implementation notes" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621427507.64" ts_lastsave="1624438373.44" unique_id="5">
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;example.py&quot;</rich_text>
		<rich_text>
(</rich_text>
		<rich_text weight="heavy">abalone clams</rich_text>
		<rich_text>, 0.1 budget)
   ☑ run
   ☑ run with info/debug
   ☑ performs DFS,BFS,3-nodes
      ◇ RMSE from 4.x - 2.7 and the alternate who's best
         </rich_text>
		<rich_text foreground="#9090eeee9090">▪ How good is a RMSE of 3?</rich_text>
		<rich_text>

</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;example_classification.py&quot;</rich_text>
		<rich_text>
</rich_text>
		<rich_text weight="heavy">classify samples from 3 random normal distributions that were scrambled</rich_text>
		<rich_text>

</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;evaluation/attack.py&quot;</rich_text>
		<rich_text>
</rich_text>
		<rich_text weight="heavy">membership</rich_text>
		<rich_text> </rich_text>
		<rich_text weight="heavy">inference</rich_text>
		<rich_text>
   ◇ not working as is

</rich_text>
		<rich_text foreground="#a0a02020f0f0">&quot;</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">results/</rich_text>
		<rich_text foreground="#a0a02020f0f0">&quot;</rich_text>
		<rich_text>
   •  n1 python plot.py's ready

    </rich_text>
		<rich_text weight="heavy">cross_val.py</rich_text>
		<rich_text>
      •   takes long
      •   --------- Processing Model DPREF, outputs only a bunch of LightGBM unknown param warnings
      • And the values I get are also much worse than the ones on github
      • so DPRef not working, skipping for now!
      • </rich_text>
		<rich_text foreground="#9090eeee9090">what is DP_Ref</rich_text>
		<rich_text>, it seems DP, but what and why is it in code. Can't find in thesis

&quot;</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">baseline</rich_text>
		<rich_text scale="h2">/</rich_text>
		<rich_text>&quot;
    code for evalutating baseliine models on the reference dataset
   •  seems ot be the non-DP code
   • Uses LightGBM, which is a paper proposing an improvement over standard GBDT 
   • grid search cross validation
   • takes really long to run

</rich_text>
		<rich_text scale="h2">“</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">model.py</rich_text>
		<rich_text scale="h2">”</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">GradientBoostingEnsemble</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DecisionNode</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">:</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">TreeExporter</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">:</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DifferentiallyPrivateTree</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>

</rich_text>
		<rich_text scale="h2">“</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">evaluation/estimator.py</rich_text>
		<rich_text scale="h2">&quot;</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DPGBDT</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">fit</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">predict</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">predict_proba</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">decision_path</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DPRef</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>
   ◇ similar
uses resp. wraps around the model.py stuff. </rich_text>
		<rich_text foreground="#ffffa5a50000">It's to use sklearn.cross_val on a DPGBDT object.</rich_text>
		<rich_text>

--------------------------------------------------------

☐ test print decision tree functionality
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">TreeExporter</rich_text>
		<rich_text> in model.py
   ◇ no usage in project
   ◇ creates attributes such that sklearn.tree.plot_tree could plot it

============================

Big picture

• translate to CPP, using libraries where possible


Questions
• </rich_text>
		<rich_text foreground="#9090eeee9090">How are we gonna identify secret dependant stuff (-&gt; replace  library code with my own there)</rich_text>
		<rich_text>
• so far the code seems to make sense, is well commentated
• maybe give me theos information and I'll question him as rquired.


--------------------------------------
go through the 2 pseudocode algorithms in 
make shared google docs with comments at each step whether its secret dependant.


gonna be interesting how fast the CPP is, since Python used automatic sklearn multithreading (4 threads on my laptop)</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="creating a single tree" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621853459.74" ts_lastsave="1625738065.82" unique_id="7">
			<rich_text scale="h1">creating a single tree</rich_text>
			<rich_text>

ressources to check before start:

• implementation ideas from theo
• implementation ideas from github_gbdt
• Implementation ideas from the internet
   ◇ n1 Makefile </rich_text>
			<rich_text link="webs https://github.com/qiyiping/gbdt/blob/master/src/cpp/Makefile">https://github.com/qiyiping/gbdt/blob/master/src/cpp/Makefile</rich_text>
			<rich_text>
   ◇ smaller, better overview decision tree repo, but Chinese comments
      ▪ </rich_text>
			<rich_text link="webs https://github.com/zhaoxingfeng/XGBoost-cpp/blob/master/src/decision_tree.cpp">https://github.com/zhaoxingfeng/XGBoost-cpp/blob/master/src/decision_tree.cpp</rich_text>
			<rich_text>
      ▪ </rich_text>
			<rich_text link="webs https://github.com/HrBlack/GBDT/blob/master/decision_tree.cpp">https://github.com/HrBlack/GBDT/blob/master/decision_tree.cpp</rich_text>
			<rich_text>
• stuff from the paper</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="questions 1.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622108984.01" ts_lastsave="1625738000.84" unique_id="8">
			<rich_text>================================
</rich_text>
			<rich_text scale="h2">Early implementation questions, before py and cpp were equal</rich_text>
			<rich_text>
================================


</rich_text>
			<rich_text foreground="#9090eeee9090">start with bfs / dfs / 2-nodes ?   Need all 3 in final implementation?</rich_text>
			<rich_text>
   •  2-nodes was best
   </rich_text>
			<rich_text foreground="#ffffa5a50000">• yeah want all</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">which dataset am I working with? synthetic I guess?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">all, but synthetic will later be the focus</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">multi classification necessary?</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffa5a50000"> if some dataset requires it, yes, because we want all the abalones etc</rich_text>
			<rich_text>

-------------------------------------
</rich_text>
			<rich_text foreground="#9090eeee9090">which options to implement</rich_text>
			<rich_text>
• </rich_text>
			<rich_text foreground="#ffffa5a50000">we want almost all of them (Esfandiar)</rich_text>
			<rich_text>
• bfs / dfs / 2-nodes (best, mostly)
   ◇ Default I'll do 
      ▪ 2-nodes, (with Leaf clipping and gradient filtering)
      ▪ balance partition = yes  (aka same number of trees per ensemble)
• Looking at it it all makes sense, or is not much overhead for me anyway
• questionable
   ◇ use_decay (&quot;</rich_text>
			<rich_text foreground="#2aa198">internal node privacy budget has a decaying factor</rich_text>
			<rich_text>&quot;)
      ▪ by default off, in results/  it's also never used.
      ▪ so this probably means it was not useful resp “can worsen accuracy” according to GLC section of base paper
      ▪ but implementation overhead is tiny.

• “according to thesis” GDF (filtering) can be selectively enabled.
   ◇ </rich_text>
			<rich_text foreground="#ffffa5a50000">the instances with a very large gradient are often outliers in the training data set since they cannot be well learned by GBDTs. Thus, it is reasonable to learn a tree by filtering those</rich_text>
			<rich_text>
outliers.
   ◇ simple option model.py line ~295
   ◇ can later test whether it makes any difference
      ▪ leave away for now.


-------------------------------------------------
</rich_text>
			<rich_text foreground="#9090eeee9090">do you see any difference in this approach vs creating all 50 sets at once</rich_text>
			<rich_text>
    “unused samples are not put back”
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar sees privacy dangers in this approach</rich_text>
			<rich_text>
        </rich_text>
			<rich_text foreground="#ffffa5a50000">if there's a tree whose ability to improve the model (discard tree or not) depends on 1 point</rich_text>
			<rich_text>
            then someone might learn something somehow. Esfandiar thinks so.
            maybe an adversary could recognize whether a sample “put back” resp. used again

</rich_text>
			<rich_text foreground="#9090eeee9090">is it normal for GBDT that you end up using less tree than specified?</rich_text>
			<rich_text>
   •  when tested, out of 50 possible trees it ended up being ~25
   •  so half of the training data was not &quot;used&quot; 
   •  but we still paid privacy budget for it
</rich_text>
			<rich_text foreground="#ffffa5a50000">   • It's not normal, but it makes sense because of DP! Randomization can create useless trees</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">Do you think that leaks information if we don't keep trees that are not used for prediction later?
    </rich_text>
			<rich_text foreground="#ffffa5a50000">YES, must keep all trees! (Esfandiar)</rich_text>
			<rich_text foreground="#9090eeee9090">

</rich_text>
			<rich_text foreground="#ffffffffffff">==============================</rich_text>
			<rich_text foreground="#9090eeee9090">

try with larger privacy budget, see if not so many trees are discarded
</rich_text>
			<rich_text>
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="questions 2.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622471704.32" ts_lastsave="1625738004.23" unique_id="12">
			<rich_text foreground="#ffffc0c0cbcb">TODO: debug the result of the pick-formula (algo 2, line 8, base paper)
    does it stay constant across trees?
    is there an easier formula that calculates the same if we have only 1 ensemble ?</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#9090eeee9090">do we even need a test set for the “real” hardened application?</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">TODO read check out theos meeting slides

TODO   </rich_text>
			<rich_text>is use_decay  used?

</rich_text>
			<rich_text foreground="#9090eeee9090">Only y is scaled right? because we only will add noise to predictions. Right? Or do we need to scale X before training? Or does it not make a difference?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar says only the labels (y)</rich_text>
			<rich_text>

Do we need to look at RMSE % compared to output range? 
    
</rich_text>
			<rich_text foreground="#ffffffff0000">I don't understand why scaling y in example.py gets such a low RMSE compared to cross_val_score</rich_text>
			<rich_text>
        it has the same contents as y in cross_val (and cpp)
        idk.
        ignore for now.
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="scratch 1.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622451203.85" ts_lastsave="1625738159.26" unique_id="11">
			<rich_text>
================================
</rich_text>
			<rich_text scale="h2">Notes up to first equal deterministic py &lt;--&gt; cpp (abalone)</rich_text>
			<rich_text>
================================

Idk why Theos abalone 5000 graph in thesis has a 0.1pb RMSE of 20-25.
   </rich_text>
			<rich_text foreground="#ffffa5a50000">   ◇  it's because of MinMaxScaler!!!</rich_text>
			<rich_text>
      ◇ its necessary, does that solve this now?

</rich_text>
			<rich_text foreground="#ffffffff0000">BFS balance_partition=False   is broken!!</rich_text>
			<rich_text> 
   ◇ threading hangs in queue.get() empty something

• balance_partition= True makes less trees rejected in 2ndsplit

</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">don't need non-dp for debug. can just “disable the exp-machanism”
</rich_text>
			<rich_text>    need to check why theo had so many if(no_dp)'s
      •   </rich_text>
			<rich_text foreground="#ffffa5a50000">mainly for privacy budget stuff</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">checkout easygdb &amp; other github gain computation.</rich_text>
			<rich_text> 
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">not making me schlau</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">check out theos thesis and base paper for formula</rich_text>
			<rich_text>
   •  </rich_text>
			<rich_text foreground="#9090eeee9090">Gain formula seems correct</rich_text>
			<rich_text foreground="#ffffa5a50000">
</rich_text>
			<rich_text foreground="#ffffffffffff">   •  EasyXGB uses &lt;= to split tough, github_gbdt and theo use &lt;
   •  trying out both shows that the gain seems to be exactly the same, but the split values differ. It should not matter.</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">    
so what is this weird </rich_text>
			<rich_text foreground="#ffffa5a50000">theo-if-binary-split-dont-consider-2nd</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">?
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">t</rich_text>
			<rich_text foreground="#9090eeee9090">ested it (by printing gainz in theos). it's only correct for categorical</rich_text>
			<rich_text foreground="#ffffa5a50000">
      ◇ so it's mostly wrong and the “savings” in categorical is small. (at least for abalone)</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffffffffffff">◇ However, noifbinary might have a similar effect as pruning, so it might not be so bad for the algorithm actually</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffffffffff">• testing now the accuracy implications in python!!!</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">  ◇ </rich_text>
			<rich_text foreground="#ffff00000000">noifbinary</rich_text>
			<rich_text foreground="#ffffa5a50000"> has negligible effect for abalone</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">print all gains and analyze pattern (seems like they stay weirdly constant across feature values)
</rich_text>
			<rich_text foreground="#ffffffffffff">   •  there's always only a limited number of possible split outcomes depending on the #samples. -&gt; for (2 (nonequal-value) samples, and &lt;=) there always has to be 1 in LHS or 2 in LHS. So 2 possible outcomes and thus 2 possible gains. And if we already computed those  values in feature 1 of 7, then the remaining 6 computations are redundant, won't get a better gain.</rich_text>
			<rich_text foreground="#ffffa5a50000">
</rich_text>
			<rich_text foreground="#9090eeee9090">   • But I would consider it a small optimization, but only to be potentially considered much later.
</rich_text>
			<rich_text foreground="#ffffffffffff">      ◇ It only seems viable for low number of sample splits.
</rich_text>
			<rich_text>   </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">
print trees (cpp and python) for easy validation</rich_text>
			<rich_text>
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">took the EasyXGB tree.cpp one</rich_text>
			<rich_text>

-------------

</rich_text>
			<rich_text scale="h2" underline="single">predict</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#ffffffff0000">theos code also passes the “leftover” samples to predict. (After giving each tree e.g. 66)
  </rich_text>
			<rich_text foreground="#ffffffffffff"> •  But it should not matter in theory.</rich_text>
			<rich_text foreground="#ffffffff0000">
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffff00000000">somehow the leaf_clipping = True does not make it to the point where it's used.</rich_text>
			<rich_text foreground="#9090eeee9090">
</rich_text>
			<rich_text>   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">it's not passed when the actual tree is built. great.</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffffff0000">• let's hope it was not a “convenient accident”</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffff00000000">◇ </rich_text>
			<rich_text>python bench: definitely noticeable for very small privacy budgets
         ▪ (like 130 instead of 120 for pb=0.1)
</rich_text>
			<rich_text foreground="#ffffffffffff">         ▪ doing it again for verification</rich_text>
			<rich_text>
            </rich_text>
			<rich_text foreground="#ffffffffffff">- 2nd run showed no difference</rich_text>
			<rich_text foreground="#ffffa5a50000"> </rich_text>
			<rich_text foreground="#9090eeee9090">-&gt; small to no effect</rich_text>
			<rich_text>
     </rich_text>
			<rich_text foreground="#ffffffff0000"> ◇ let's see effect on 2nd split</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffffffffff">         ▪ DFS got better, 2-nodes got worse. Weird.
</rich_text>
			<rich_text>            - doing it again for verification.
</rich_text>
			<rich_text foreground="#ffffffffffff">            - 2nd try looks almost identical. So kinda false alarm</rich_text>
			<rich_text>
     </rich_text>
			<rich_text foreground="#ffff00000000"> </rich_text>
			<rich_text foreground="#ffffa5a50000">◇ is it really required by the proof?</rich_text>
			<rich_text>
         ▪ yeah proof bounds seem to build on it
   • at least the gradient_filtering is used by the code.


</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">need to read the results part of the base paper, to better understand the RMSE
numbers and leaf clipping stuff. And the effect of having multiple ensembles</rich_text>
			<rich_text>
   •  skip for now?</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">does DPBoost (base paper) have a github?
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">yup </rich_text>
			<rich_text foreground="#ffffa5a50000" link="webs https://github.com/QinbinLi/DPBoost/">https://github.com/QinbinLi/DPBoost/</rich_text>
			<rich_text foreground="#ffffa5a50000"> 
   •  but code example broken and in general it's messy
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#9090eeee9090">    
</rich_text>
			<rich_text foreground="#ffffffff0000">why the fuck are the base paper absolute values different from theos?</rich_text>
			<rich_text foreground="#9090eeee9090">
</rich_text>
			<rich_text>   • </rich_text>
			<rich_text foreground="#9090eeee9090">is it because they use budget like 0.5 as smallest and theo 0.1?
   • yes and also because I misread the log plot!</rich_text>
			<rich_text>
   • is it because theo only has 1 ensemble?? (&quot;Boosting effect?&quot;)
      ◇ there's some theo code for 1+ ensembles. But Theo didn't use it so it probably doesn't work.
   •</rich_text>
			<rich_text foreground="#ffffa5a50000"> nope, there are DPBoost 1 ensemble graphs.</rich_text>
			<rich_text>


============================

had differences with more samples being use for prediction in theo code
   • </rich_text>
			<rich_text foreground="#ffffffff0000">made them both start with 3300 and ignore some samples</rich_text>
			<rich_text>
      ◇ floor( train set / 50 )

ERROR SOMEWHERE, INVESTIGATE TODAY

            • </rich_text>
			<rich_text foreground="#9090eeee9090">it was because cpp gradient_filtering was disabled. python does [-1,1]</rich_text>
			<rich_text>
            
now the tree diffs are identical (on first glance), predictions seem to match

============================

</rich_text>
			<rich_text scale="h2">scores.</rich_text>
			<rich_text>

so the predictions from both py and cpp have the same values, and their sum is equal (-0.841) so why the fuck does the py code go to -8?
   •  first forgot the init_value
   •  because of the inverse MinMaxScaler still wrong
      ◇ where is that -1.25 from
      ◇ ok, took inverse scaliing code from MinMaxScaler

</rich_text>
			<rich_text foreground="#9090eeee9090">laplace distribution sampling seems fine (tested with python plot in ./stuff)
</rich_text>
			<rich_text>

============================

</rich_text>
			<rich_text scale="h2">scores -&gt; float accuracy issues</rich_text>
			<rich_text>


</rich_text>
			<rich_text foreground="#ffffa5a50000">looks like predictions are slightly off (some kind of precision errors)</rich_text>
			<rich_text>
   •  can at some point affect splits which in turn can change the entire process
      ◇ Happened in (1 of the 5 abalone cross_val)
    
    python results/abalone/test.py 
        [3.43531704 </rich_text>
			<rich_text foreground="#ffffffff0000">1.63171827 </rich_text>
			<rich_text>2.76654838 2.17902237 2.26754763]
        
    python gain-clip
        [3.43531704 </rich_text>
			<rich_text foreground="#9090eeee9090">1.62848293</rich_text>
			<rich_text> 2.76654838 2.17902237 2.26754763]
            ▪  cpp does not seem to need the same gain clip

    cpp double
        [3.43531704 </rich_text>
			<rich_text foreground="#9090eeee9090">1.62848293</rich_text>
			<rich_text> 2.76654838 2.17902237 2.26754763]
               •  it's not an exp mechanism overflow problem (obviously, deterministic)
        
</rich_text>
			<rich_text foreground="#ffffffff0000">        
Theo weirdly rounds privacy_budget_for node to 7 decimals for some reason!
         </rich_text>
			<rich_text foreground="#ffffffffffff">→ fixed, but had no effect on final result</rich_text>
			<rich_text foreground="#ffffffff0000">


</rich_text>
			<rich_text foreground="#ffffffffffff">• Shit, already recursive_print_tree   tree 1 split 4 differs
   ◇  (1,0.375) cpp    vs    (2,0.280) py
      ▪ have the exact same gain(s) !!!!!!!!!!   </rich_text>
			<rich_text foreground="#ffffffff0000">
</rich_text>
			<rich_text foreground="#ffffffffffff">      ▪ even though the splits are different I think (both 4:1 but different one in rhs)
   ◇  cpp max_elem will choose the first one.
   ◇ Python one should too (uses “smaller than”)</rich_text>
			<rich_text foreground="#ffffffff0000">
        PROBLEM in PYTHON the floats are not exactly equal, in cpp they appear to be!
            </rich_text>
			<rich_text foreground="#ffffffffffff">0.0303030410466746357</rich_text>
			<rich_text foreground="#ffff00000000">75</rich_text>
			<rich_text foreground="#ffffffff0000">  </rich_text>
			<rich_text foreground="#ffffffffffff">and</rich_text>
			<rich_text foreground="#ffffffff0000">
            </rich_text>
			<rich_text foreground="#ffffffffffff">0.0303030410466746357</rich_text>
			<rich_text foreground="#ffff00000000">62</rich_text>
			<rich_text>
        stemming from gains that are again off by a tiny bit
            0.000625254285352182</rich_text>
			<rich_text foreground="#ffff00000000">5</rich_text>
			<rich_text>
            0.000625254285352182</rich_text>
			<rich_text foreground="#ffff00000000">7</rich_text>
			<rich_text>
            
   •  finding the max probability with a tolerance for close values with numpy.close failed (rtol and atol 1e-10 to 1e-19)
         ▪ first ensemble all messed up again
   •  </rich_text>
			<rich_text foreground="#9090eeee9090">now trying to cap the (py) gain with floor(x*1e10)/1e10</rich_text>
			<rich_text>
         ▪ worked for abalone, deterministic py and cpp now equal</rich_text>
			<rich_text foreground="#ffffffff0000">


</rich_text>
			<rich_text foreground="#ffffa5a50000">So.... was this a bug in python and where did it come from?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#9090eeee9090">fuck this for now.</rich_text>
			<rich_text>
    the gain was already wrong, would need to go down the rabbit hole, maybe the gradients are the culprit</rich_text>
			<rich_text foreground="#ffffffff0000">
</rich_text>
			<rich_text>



</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="scratch 2.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1625738159.26" ts_lastsave="1626104846.66" unique_id="22">
			<rich_text scale="h2">Validation</rich_text>
			<rich_text>

</rich_text>
			<rich_text scale="h3">Preparations</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">• restructure repo like code -&gt; cpp, python</rich_text>
			<rich_text>

   ◇ restructure python code (less folders if possible)
</rich_text>
			<rich_text foreground="#9090eeee9090">      ▪ also move out datasets
         and have symlink inside (to be able to run the individuals from there)</rich_text>
			<rich_text>


</rich_text>
			<rich_text foreground="#9090eeee9090">Then the code needs to parse a runtime argument
    that will be stored in some global var.
    This will output LEAFSUM-like stuff, as well as RMSEs
        for tracing down where differences might arise.</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#9090eeee9090">   •  probably also make the DETERMINISM FLAG runtime arg at some point</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">need validation/testcases
        smaller versions of the dataset(s)
        start with abalone
            C++</rich_text>
			<rich_text>
            python
        
</rich_text>
			<rich_text foreground="#9090eeee9090">need verification/run_py_gbdt.py     repr “verification.py”
need verification/verification.cpp   repr “abalone.cpp”
   •  both need to write text output  to verification/outputs</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">validation/validate.cpp that parses, compares and colors the output diff
   •  maybe easier in python?
   </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">• so far rudimentary console diff</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">code/verify.sh that runs it all (compile cpp first)
   </rich_text>
			<rich_text foreground="#ffffffffffff">•  need to adapt such that it runs the same name pairs</rich_text>
			<rich_text foreground="#9090eeee9090">
</rich_text>
			<rich_text>



TODOOOO, seems like giovanna's github is very clean compared to ppml, so can ausmisten. maxbe ask theo in the end whether there is chlüg stuff to put in</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="DPBoost" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624435164.19" ts_lastsave="1624435968.45" unique_id="18">
			<rich_text>• GDF suffices for the delta_g and delta_v formulas that we use
   ◇ So leaf_clipping seems optional?
   ◇ And the way theo implemented GDF seems wrong??
      ▪ GDF provides same sensitivities across all trees, unlike GLC.

• GLC's sensitivity bound gets tighter with the iterations

• Then the leaf node sensitivity bounds can be combined when using both
   ◇ delta_v = min(sens GDF, sens GLC)
   ◇ sensitivity (&quot;range of function&quot;) determines how much noise has to be added for DP</rich_text>
		</node>
	</node>
	<node custom_icon_id="0" foreground="#0000ff" is_bold="False" name="Further TODOs" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624438358.97" ts_lastsave="1625836698.83" unique_id="19">
		<rich_text scale="h2">Algorithm</rich_text>
		<rich_text>

• need to fix the sample choice to use the paper formula

• my </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">GDF</rich_text>
		<rich_text> is like theo's, but is that what DPBoost / paper says?

•  right now i'm splitting with “&lt;” like theo. 
   ◇ in _predict as well as in </rich_text>
		<rich_text foreground="#268bd2">samples_left_right_partition</rich_text>
		<rich_text>
   ◇ easyXGB does &lt;=. however it does not affect the gain. But still unsure what's right.
   ◇ would be nice to define the operation at 1 point

• my </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">2-sample-gain-split</rich_text>
		<rich_text> 2 -&gt; (1,1)
   ◇ compare that to DPBoost?
   ◇ Or does it not really matter for larger datasets? Hold for now.

• </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">Use_decay</rich_text>
		<rich_text>. (pb for internal nodes, “because upper splits are more important”) according to Theo thesis it can be nice, but for deep trees it gets bad. Need to check for which datasets it makes sense.

• </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">2-nodes</rich_text>
		<rich_text>: is it worth it? apparently it should be good for small amount of samples (according to theo meeing notes)

</rich_text>
		<rich_text scale="h2">CPP Bonus</rich_text>
		<rich_text>
• read style guide

• stackoverflow code review?

• try on a fresh ubuntu VM to get the setup / installs / requirements.txt

</rich_text>
		<rich_text scale="h2">Bonus*</rich_text>
		<rich_text>

• Some docs on the cpp algorithm
   ◇ like intuition which parameters are suitable for which use case</rich_text>
	</node>
	<node custom_icon_id="0" foreground="" is_bold="False" name="technical problems" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1625648902.91" ts_lastsave="1626169666.86" unique_id="21">
		<rich_text>Suddenly python problem (after apt update/upgrade I believe).
      </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">File &quot;/home/loretanr/.local/lib/python3.7/site-packages/matplotlib/__init__.py&quot;, line 107, in &lt;module&gt;
    from . import _api, cbook, docstring, rcsetup
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked
      File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 724, in exec_module
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 857, in get_code
      File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 525, in _compile_bytecode
    </rich_text>
		<rich_text foreground="#ffff00000000">ValueError: bad marshal data (invalid reference)</rich_text>
		<rich_text>

import lightgbm was failing
    because import matplotlib was failing
        had to delete .pyc files with 
            sudo find </rich_text>
		<rich_text link="fold L2hvbWUvbG9yZXRhbnIvLmxvY2FsL2xpYi9weXRob24zLjcvc2l0ZS1wYWNrYWdlcy9tYXRwbG90bGli">/home/loretanr/.local/lib/python3.7/site-packages/matplotlib</rich_text>
		<rich_text> -name '*.pyc' -delete
        as described in </rich_text>
		<rich_text link="webs https://www.py4u.net/discuss/17141">https://www.py4u.net/discuss/17141</rich_text>
		<rich_text>
        
-------------------------------------------------------------------------------------------------------


stuuuuupid    </rich_text>
		<rich_text family="monospace">'/usr/lib/python37.zip': No such file or directory</rich_text>
		<rich_text>
        because I clicked on breakpoints “Raised Exceptions” in VScode
        
-------------------------------------------------------------------------------------------------------</rich_text>
	</node>
</cherrytree>
