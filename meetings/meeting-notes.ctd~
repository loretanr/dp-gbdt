<?xml version="1.0" ?>
<cherrytree>
	<node custom_icon_id="0" foreground="" is_bold="True" name="Meeting notes" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619168425.57" ts_lastsave="1624438358.97" unique_id="1">
		<rich_text>Thu 3pm
Link:  </rich_text>
		<rich_text foreground="#3d3c40"> </rich_text>
		<rich_text link="webs https://ethz.zoom.us/j/4333973638">https://ethz.zoom.us/j/4333973638</rich_text>
		<rich_text>



</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="04_22_first" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619168455.86" ts_lastsave="1619595361.65" unique_id="2">
			<rich_text scale="h2">Discussion points</rich_text>
			<rich_text>

More details on the tasks in the thesis description:

• Task 0
   ◇ ~2 weeks
   ◇ Also hello world enclave in SGX dev environment, with i/o and r/w from disk

• Task 3
   ◇ depends on the results from 1 and 2

• Task 4
   ◇ Running the algorithm multiple times on the same input, can weaken the randomness/privacy guarantees of the privacy-preserving part. we need to avoid that. Kari and Esfandiari have ideas.

• Task 5
   ◇ we'll see how much time we have for this

• Task ...
   ◇ Where “real research” resp new things would happen
   ◇ This could very well be the hardest task if we get here

• Task n
   ◇ 1 month for writing


</rich_text>
			<rich_text scale="h2">Organization</rich_text>
			<rich_text>

• Weekly thursday meetings 3PM
   ◇ maybe daily updates to mattermost?


</rich_text>
			<rich_text scale="h2">Next steps</rich_text>
			<rich_text>

• Read about the ML algorithm
• Fully read the papers
• Hello world enclave as described above.
• prepare eth paperwork for thesis start
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="04_29" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1619595361.65" ts_lastsave="1620053169.37" unique_id="3">
			<rich_text scale="h2">What I did the past week</rich_text>
			<rich_text>

• Reading “Intel SGX explained” paper (120 pages, from MIT)
   ◇ gone through background on computer architecture (CPU and memory)

• Read Theos thesis (except synthetic data generation)
   ◇ understand the idea of  Decision Trees and Gradient Boosting, Differential privacy
      ▪ how exactly the formulas are derived, not clear

• Working/backup private github repo
   ◇ because no permission for ethz gitlab
   ◇ add and share link with you in mettermost?
      ▪ would need github names
   ◇ Time log excel
   ◇ </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">write in mattermost whenever I finished a task</rich_text>
			<rich_text>
      ▪ then you can add something if you want

• Read side channel papers
   ◇ finished 2/6 (oldest ones)

• can run enclaves in my OS
   ◇ not yet created my own enclave
   ◇ but checked out and ran the SampleEnclave that installation comes with

----------------------------- still TODO ----------------------------------

• read the GBDT paper
• run theos algorithm to understand it better
   ◇ compare code and thesis
   ◇ understand what's new compared to the GBDT paper
• need to read the rest of the side-channel papers
• play around with the hello world enclave

-------------------------------------------------------------------------------

</rich_text>
			<rich_text scale="h2">Questions</rich_text>
			<rich_text>   </rich_text>
			<rich_text background="#ffffffff0000" foreground="#1a1a1a1a1a1a">TODO</rich_text>
			<rich_text>

• Looking at the the python code.... when generating a cpp algorithm
   ◇ math, ML and other libraries?
      ▪ just use whatever cpp libraries I can find?
      ▪ or what's the strategy here
      ▪ other recommandations from you guys, maybe you've already done it?

==========================================

</rich_text>
			<rich_text scale="h2">Discussion Points</rich_text>
			<rich_text>

• Order for reading papers:
   ◇ </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1ht7h--tp0ivNH9Z8S1tpoRoyzN_Q7DVU3sHaoRdDktw/edit?ts=608aacbd#">https://docs.google.com/document/d/1ht7h--tp0ivNH9Z8S1tpoRoyzN_Q7DVU3sHaoRdDktw/edit?ts=608aacbd#</rich_text>
			<rich_text>
   ◇ skip rollback paper for now

• Take notes when reading!

• ML: do Tutorials

• Theo could explain his code

• Can officially start mid May

</rich_text>
			<rich_text scale="h3">Next steps</rich_text>
			<rich_text>

• Read read read and make notes
• submit thesis proposal</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_06" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1620053169.39" ts_lastsave="1620308160.81" unique_id="4">
			<rich_text scale="h2">Questions</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">DP-GBDT proposes 2 level structure Ensemble of Ensembles.</rich_text>
			<rich_text>
   ◇ The first step splits the input data trees are created using parallel composition
   ◇ 2nd step would create multiple such ensembles and do sequential composition
      ▪ this is left away in Theos thesis, right? Was there a reason (besides time maybe)
      ▪ if an enclave would do multiple such 2nd steps that would be similar to the adversary doing rollback attack
</rich_text>
			<rich_text foreground="#ffffa5a50000">• It was sufficient to do the inner level
• and by doing the outer you would gain accurracy and lose privacy</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">How can the insurance now learn from the model?</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffa5a50000">• The trees are not useless, need to check evaluation/appendix of thesis
• especially the inner nodes have a good probability to be the most important ones on top etc.</rich_text>
			<rich_text>

Let's say we need 500 companies' data for a meaningful model. Until we collected that many, the questionnaires are not useful. So we would need to store them somewhere. (encrypted etc probably).
   ◇ And when we have 500 we feed it to the enclave
   ◇ </rich_text>
			<rich_text foreground="#9090eeee9090">What if we feed 400 distinct, and then 100x the same company, or maybe some other clever pattern?</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffa5a50000">• we need to define those things, and specify such rules in the enclave code (attestation)
• Or there may be some approaches where the data deletes itself after 1 run or something
• we kind of need to find out this kind of things</rich_text>
			<rich_text>

-------------------------------------------------------------
</rich_text>
			<rich_text scale="h3">3 options for coding</rich_text>
			<rich_text>

• \exists way to run python code in enclave -&gt; bad, much code, how to secure
• use C DPDT implementation, start from there
   ◇ </rich_text>
			<rich_text link="webs https://github.com/yarny/gbdt">https://github.com/yarny/gbdt</rich_text>
			<rich_text>
   ◇ however this code is (highly) optimized
• </rich_text>
			<rich_text foreground="#ffffc0c0cbcb">do it from scratch</rich_text>
			<rich_text>
   ◇ can use libraries as much as we want for non-secret dependant parts
   ◇ but do the secret dependant parts on my own
      ▪ then we know them exactly

------------- next steps -------------------
• finish reading
• Tell Kari to setup a meeting with Theo as soon as I'm ready
• get to know Theos code

</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_20" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621517715.69" ts_lastsave="1621517939.67" unique_id="6">
			<rich_text>• we'll try and find secret dependant accesses by hand first!
   ◇ because tools can be messy!
   ◇ esfanidar sent a paper for such a tool </rich_text>
			<rich_text link="webs https://hal.inria.fr/hal-01658653/document">https://hal.inria.fr/hal-01658653/document</rich_text>
			<rich_text>

• TODO create a shared google docs with the 2 algorithms from the DPGDBDT paper
   ◇ (</rich_text>
			<rich_text link="webs https://ojs.aaai.org//index.php/AAAI/article/view/5422">https://ojs.aaai.org//index.php/AAAI/article/view/5422</rich_text>
			<rich_text>)
   ◇ need to be able to annotate each part with our ideas whether it is potentially risky
   ◇ and need space to insert solutions like “use list, loop through all nodes etc”</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="05_28" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622210274.9" ts_lastsave="1622213346.65" unique_id="10">
			<rich_text>
• High level analysis of the algorithm side channel was ok.
    But at some point we need a more detailed overview of the problem and how my implementation solves it.

• Had difficulties to explain the The [0,1] probabilities addition thingy. 
        The thing that Theo does to implement the exponential mechanism.
        is named something like CTF (probability distribution) with increasing balkens something.
        
        
        
</rich_text>
			<rich_text scale="h2">Q &amp; A</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">do you see any difference in this approach vs creating all 50 sets at once</rich_text>
			<rich_text>
    “unused samples are not put back”
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar sees privacy dangers in this approach</rich_text>
			<rich_text>
        </rich_text>
			<rich_text foreground="#ffffa5a50000">if there's a tree whose ability to improve the model (discard tree or not) depends on 1 point</rich_text>
			<rich_text>
            then someone might learn something somehow. Esfandiar thinks so.
            maybe an adversary could recognize whether a sample “put back” resp. used again

</rich_text>
			<rich_text foreground="#9090eeee9090">is it normal for GBDT that you end up using less tree than specified?</rich_text>
			<rich_text>
   •  when tested, out of 50 possible trees it ended up being ~25
   •  so half of the training data was not &quot;used&quot; 
   •  but we still paid privacy budget for it
</rich_text>
			<rich_text foreground="#ffffa5a50000">   • It's not normal, normal GBDT improve with every tree.
      ◇  but it makes sense because of DP! Randomization can create useless trees</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">Do you think that leaks information if we don't keep trees that are not used for prediction later?
    </rich_text>
			<rich_text foreground="#ffffa5a50000">YES, must keep all trees! (Esfandiar)</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#9090eeee9090">start with bfs / dfs / 2-nodes ?   Need all 3 in final implementation?</rich_text>
			<rich_text>
   •  2-nodes was best
   </rich_text>
			<rich_text foreground="#ffffa5a50000">• would be nice to have all options</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">which dataset am I working with? synthetic I guess?</rich_text>
			<rich_text>
       </rich_text>
			<rich_text foreground="#ffffa5a50000">• would be nice to have all abalones and datasets!</rich_text>
			<rich_text>
       
    
</rich_text>
			<rich_text scale="h2">TODO</rich_text>
			<rich_text> next steps:
   ◇ we need to check with higher privacy budget. Then less trees should get discarded.

   ◇ </rich_text>
			<rich_text weight="heavy">Remove the “put back ” strategy in python code and see whether it works as good as Theo’s code does.</rich_text>
			<rich_text>
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_03" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622727420.89" ts_lastsave="1623056086.55" unique_id="13">
			<rich_text> discussing and Q&amp;A also here
 
    </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1GaqAwNijzrrf_228VdCj4WU4N1dySOratf2DJ6VN3XI/edit#">https://docs.google.com/document/d/1GaqAwNijzrrf_228VdCj4WU4N1dySOratf2DJ6VN3XI/edit#</rich_text>
			<rich_text>
    
    
    
Q &amp; A

   •  The scaling to [-1,1] in Theos code is necessary for the proof.
      ◇ If those values would be larger we would need more noise
      ◇  the 3 in delta G sensitivity calculation (in code) is because of the scaling!


TODO
   ◇ create clean graphs
   ◇ especially one that uses alltrees and no rejection for pb 0.5 to 4
   ◇ debugger how many instances where (abalone + maybe some larger dataset?)
   ◇ put __pycache__'s in gitignore if not already.
   ◇ done </rich_text>
			<rich_text link="webs https://docs.google.com/document/d/1CEYstt6WXV2DO6hBD7ENk2zDl_5xUWXYuJB7Cd5PjUw/edit">https://docs.google.com/document/d/1CEYstt6WXV2DO6hBD7ENk2zDl_5xUWXYuJB7Cd5PjUw/edit</rich_text>
			<rich_text>

TODO 
    check out Theos meeting slides.</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_10" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1623331472.64" ts_lastsave="1623332681.42" unique_id="14">
			<rich_text foreground="#9090eeee9090">How does the mighty “proof” of Moritz look?</rich_text>
			<rich_text>
•  got the DPGBDT proof from esfandiar 
   ◇ I don't understand anything
    
</rich_text>
			<rich_text foreground="#9090eeee9090">do you have an idea of how I could create a better overview of algorithm and implementation to reason about side channel stuff?</rich_text>
			<rich_text>
   •  need some form of more detailed pseudocode (with more details)

</rich_text>
			<rich_text foreground="#9090eeee9090">Only y is scaled right? because we only will add noise to predictions. Right?
Or do we need to scale X before training? Or does it not make a difference?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar says only the labels (y)</rich_text>
			<rich_text>
    o/w read the base paper...
    
    
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="06_17" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1623939008.65" ts_lastsave="1623939129.56" unique_id="15">
			<rich_text>• need to indicate after each pseudocode, what exactly it could leak in its current form (even algorithm parameters)

• special consideration to all branches like the ones on top of find_best_split

• special consideration required for stuff like “continue;”</rich_text>
		</node>
	</node>
	<node custom_icon_id="0" foreground="#ff0000" is_bold="True" name="Implementation notes" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621427507.64" ts_lastsave="1624438373.44" unique_id="5">
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;example.py&quot;</rich_text>
		<rich_text>
(</rich_text>
		<rich_text weight="heavy">abalone clams</rich_text>
		<rich_text>, 0.1 budget)
   ☑ run
   ☑ run with info/debug
   ☑ performs DFS,BFS,3-nodes
      ◇ RMSE from 4.x - 2.7 and the alternate who's best
         </rich_text>
		<rich_text foreground="#9090eeee9090">▪ How good is a RMSE of 3?</rich_text>
		<rich_text>

</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;example_classification.py&quot;</rich_text>
		<rich_text>
</rich_text>
		<rich_text weight="heavy">classify samples from 3 random normal distributions that were scrambled</rich_text>
		<rich_text>

</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">&quot;evaluation/attack.py&quot;</rich_text>
		<rich_text>
</rich_text>
		<rich_text weight="heavy">membership</rich_text>
		<rich_text> </rich_text>
		<rich_text weight="heavy">inference</rich_text>
		<rich_text>
   ◇ not working as is

</rich_text>
		<rich_text foreground="#a0a02020f0f0">&quot;</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">results/</rich_text>
		<rich_text foreground="#a0a02020f0f0">&quot;</rich_text>
		<rich_text>
   •  n1 python plot.py's ready

    </rich_text>
		<rich_text weight="heavy">cross_val.py</rich_text>
		<rich_text>
      •   takes long
      •   --------- Processing Model DPREF, outputs only a bunch of LightGBM unknown param warnings
      • And the values I get are also much worse than the ones on github
      • so DPRef not working, skipping for now!
      • </rich_text>
		<rich_text foreground="#9090eeee9090">what is DP_Ref</rich_text>
		<rich_text>, it seems DP, but what and why is it in code. Can't find in thesis

&quot;</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">baseline</rich_text>
		<rich_text scale="h2">/</rich_text>
		<rich_text>&quot;
    code for evalutating baseliine models on the reference dataset
   •  seems ot be the non-DP code
   • Uses LightGBM, which is a paper proposing an improvement over standard GBDT 
   • grid search cross validation
   • takes really long to run

</rich_text>
		<rich_text scale="h2">“</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">model.py</rich_text>
		<rich_text scale="h2">”</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">GradientBoostingEnsemble</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DecisionNode</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">:</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">TreeExporter</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">:</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DifferentiallyPrivateTree</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>

</rich_text>
		<rich_text scale="h2">“</rich_text>
		<rich_text foreground="#a0a02020f0f0" scale="h2">evaluation/estimator.py</rich_text>
		<rich_text scale="h2">&quot;</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DPGBDT</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">fit</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">predict</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">predict_proba</rich_text>
		<rich_text>
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">def</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#268bd2" weight="heavy">decision_path</rich_text>
		<rich_text>
• </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">DPRef</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">(</rich_text>
		<rich_text foreground="#6c71c4" weight="heavy">BaseEstimator</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy">):</rich_text>
		<rich_text>
   ◇ similar
uses resp. wraps around the model.py stuff. </rich_text>
		<rich_text foreground="#ffffa5a50000">It's to use sklearn.cross_val on a DPGBDT object.</rich_text>
		<rich_text>

--------------------------------------------------------

☐ test print decision tree functionality
   ◇ </rich_text>
		<rich_text foreground="#93a1a1" weight="heavy">class</rich_text>
		<rich_text foreground="#bbbbbb" weight="heavy"> </rich_text>
		<rich_text foreground="#cb4b16" weight="heavy">TreeExporter</rich_text>
		<rich_text> in model.py
   ◇ no usage in project
   ◇ creates attributes such that sklearn.tree.plot_tree could plot it

============================

Big picture

• translate to CPP, using libraries where possible


Questions
• </rich_text>
		<rich_text foreground="#9090eeee9090">How are we gonna identify secret dependant stuff (-&gt; replace  library code with my own there)</rich_text>
		<rich_text>
• so far the code seems to make sense, is well commentated
• maybe give me theos information and I'll question him as rquired.


--------------------------------------
go through the 2 pseudocode algorithms in 
make shared google docs with comments at each step whether its secret dependant.


gonna be interesting how fast the CPP is, since Python used automatic sklearn multithreading (4 threads on my laptop)</rich_text>
		<node custom_icon_id="0" foreground="" is_bold="False" name="creating a single tree" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1621853459.74" ts_lastsave="1622032046.54" unique_id="7">
			<rich_text scale="h1">creating a single tree</rich_text>
			<rich_text>

ressources to check before start:

• implementation ideas from theo
• implementation ideas from github_gbdt
• Implementation ideas from the internet
   ◇ n1 Makefile </rich_text>
			<rich_text link="webs https://github.com/qiyiping/gbdt/blob/master/src/cpp/Makefile">https://github.com/qiyiping/gbdt/blob/master/src/cpp/Makefile</rich_text>
			<rich_text>
   ◇ smaller, better overview decision tree repo, but Chinese comments
      ▪ </rich_text>
			<rich_text link="webs https://github.com/zhaoxingfeng/XGBoost-cpp/blob/master/src/decision_tree.cpp">https://github.com/zhaoxingfeng/XGBoost-cpp/blob/master/src/decision_tree.cpp</rich_text>
			<rich_text>
      ▪ </rich_text>
			<rich_text link="webs https://github.com/HrBlack/GBDT/blob/master/decision_tree.cpp">https://github.com/HrBlack/GBDT/blob/master/decision_tree.cpp</rich_text>
			<rich_text>
   ◇ 
• stuff from the paper



==============================
What I need:
   •  train() / fit()
      ◇ loop to grow_tree() each tree in the ensemble
   • grow_tree()
      ◇ XGBoost uses a queue
      ◇ HrBlack/GBDT uses 2 lists that he pushes into
      ◇ XGBoost-cpp uses class Tree with attributes like split_val, left_child etc.
      ◇ Theo builds it recursively with MakeTreeBFS/DFS -&gt; create nodes and point to the children etc.
         ▪ the logic is all there, no ML library used.</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="questions" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622108984.01" ts_lastsave="1622213562.55" unique_id="8">
			<rich_text foreground="#9090eeee9090">start with bfs / dfs / 2-nodes ?   Need all 3 in final implementation?</rich_text>
			<rich_text>
   •  2-nodes was best
   </rich_text>
			<rich_text foreground="#ffffa5a50000">• yeah want all</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">which dataset am I working with? synthetic I guess?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">all, but synthetic will later be the focus</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">multi classification necessary?</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffa5a50000"> if some dataset requires it, yes, because we want all the abalones etc</rich_text>
			<rich_text>

-------------------------------------
</rich_text>
			<rich_text foreground="#9090eeee9090">which options to implement</rich_text>
			<rich_text>
• </rich_text>
			<rich_text foreground="#ffffa5a50000">we want almost all of them (Esfandiar)</rich_text>
			<rich_text>
• bfs / dfs / 2-nodes (best, mostly)
   ◇ Default I'll do 
      ▪ 2-nodes, (with Leaf clipping and gradient filtering)
      ▪ balance partition = yes  (aka same number of trees per ensemble)
• Looking at it it all makes sense, or is not much overhead for me anyway
• questionable
   ◇ use_decay (&quot;</rich_text>
			<rich_text foreground="#2aa198">internal node privacy budget has a decaying factor</rich_text>
			<rich_text>&quot;)
      ▪ by default off, in results/  it's also never used.
      ▪ so this probably means it was not useful resp “can worsen accuracy” according to GLC section of base paper
      ▪ but implementation overhead is tiny.

• “according to thesis” GDF (filtering) can be selectively enabled.
   ◇ </rich_text>
			<rich_text foreground="#ffffa5a50000">the instances with a very large gradient are often outliers in the training data set since they cannot be well learned by GBDTs. Thus, it is reasonable to learn a tree by filtering those</rich_text>
			<rich_text>
outliers.
   ◇ simple option model.py line ~295
   ◇ can later test whether it makes any difference
      ▪ leave away for now.


-------------------------------------------------
</rich_text>
			<rich_text foreground="#9090eeee9090">do you see any difference in this approach vs creating all 50 sets at once</rich_text>
			<rich_text>
    “unused samples are not put back”
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar sees privacy dangers in this approach</rich_text>
			<rich_text>
        </rich_text>
			<rich_text foreground="#ffffa5a50000">if there's a tree whose ability to improve the model (discard tree or not) depends on 1 point</rich_text>
			<rich_text>
            then someone might learn something somehow. Esfandiar thinks so.
            maybe an adversary could recognize whether a sample “put back” resp. used again

</rich_text>
			<rich_text foreground="#9090eeee9090">is it normal for GBDT that you end up using less tree than specified?</rich_text>
			<rich_text>
   •  when tested, out of 50 possible trees it ended up being ~25
   •  so half of the training data was not &quot;used&quot; 
   •  but we still paid privacy budget for it
</rich_text>
			<rich_text foreground="#ffffa5a50000">   • It's not normal, but it makes sense because of DP! Randomization can create useless trees</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#9090eeee9090">Do you think that leaks information if we don't keep trees that are not used for prediction later?
    </rich_text>
			<rich_text foreground="#ffffa5a50000">YES, must keep all trees! (Esfandiar)</rich_text>
			<rich_text foreground="#9090eeee9090">

</rich_text>
			<rich_text foreground="#ffffffffffff">==============================</rich_text>
			<rich_text foreground="#9090eeee9090">

try with larger privacy budget, see if not so many trees are discarded
</rich_text>
			<rich_text>
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="ensembles" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622123435.95" ts_lastsave="1622213662.62" unique_id="9">
			<rich_text>Train()
   •  info-logs the number of ensembles
   • called by estimator. (DPGBT).fit()
   • Then all trees get trained
      ◇ and they are assigned to their ensemble by   index modulo #ensembles
   • Theos code does 50 trees 1 ensemble for the 5000 samples case (at least for abalone)
      ◇ for the 300 sample case he uses 5 trees and ignores the nb_trees_per_ensemble

fit()
   •  called by directly after chearing model = (, , , , , ,) model.fit(X,y)
   • example.py only creates 1 (50 tree) ensemble
   • cross_val he would do 50 trees per ensemble



</rich_text>
			<rich_text scale="h2">Train()</rich_text>
			<rich_text>
• so theo does like an ensemble of 50 trees for like 3000 samples.
• so each gets 60 instances.
• Theo randomly selects 60 and checks if the tree improves the ensembles least squares error score.
   ◇ If not, he “skips” that tree (by not even saving it) 
      ▪ its training instances though are keptl usable for the next ones.
   ◇ So the number of trees (out of those 50) is variable and depends on the run! (like 20-30) out of 50

</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="scratch" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622451203.85" ts_lastsave="1624981746.24" unique_id="11">
			<rich_text>Looking at the example.py results.
   •  so, somehow BFS is either bad or wrong implemented
        probably why it Theo does not mention it in thesis
   • Idk why Theos abalone 5000 graph in thesis has a 0.1pb RMSE of 20-25.
</rich_text>
			<rich_text foreground="#ffffa5a50000">   •  it's because of MinMaxScaler!!!</rich_text>
			<rich_text>


BFS balance_partition=False   is broken!! threading hangs in queue.get() empty something

balance_partition= True makes less trees rejected in 2ndsplit

would be nice to have cross_val in C++,
    it would not need to be side-channel proof
    it's just validation

</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">don't need non-dp for debug. can just “disable the exp-machanism”
</rich_text>
			<rich_text>    need to check why theo had so many if(no_dp)'s
      •   </rich_text>
			<rich_text foreground="#ffffa5a50000">mainly for privacy budget stuff
      •   if I want a correct non-dp, would need to think about it more thoroughly (e.g. does it affect the tree building or can we just flip the switch in exp mechanism and leaf-add-laplace-noise? And not care about the redundant computation. This could be low hanging fruit)</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">checkout easygdb &amp; other github gain computation.</rich_text>
			<rich_text> 
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">not making me schlau</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">check out theos thesis and base paper for formula</rich_text>
			<rich_text>
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">Gain formula seems correct
   •  EasyXGB uses &lt;= instead of &lt; to split tough
   •  github_gbdt uses &lt;
   •  trying out both shows that the gain seems to be exactly the same, but the split values differ. It should not matter.</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">    
so what is this weird theo-if-binary-split-dont-consider-2nd?
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">tested it (by printing gainz in theos). it's only correct for categorical, so it's mostly wrong and the “savings” in categorical is small. (at least for abalone)</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">print all gains and analyze pattern (seems like they stay weirdly constant across feature values)
</rich_text>
			<rich_text foreground="#ffffa5a50000">   •  there's always only a limited number of possible split outcomes depending on the #samples. -&gt; for (2 (nonequal-value) samples, and &lt;=) there always has to be 1 in LHS or 2 in LHS. So 2 possible outcomes and thus 2 possible gains. And if we already computed those  values in feature 1 of 7, then the remaining 6 computations are redundant, won't get a better gain.
   • But I would consider it a small optimization, but only to be potentially considered much later.
   • It only seems viable for low number of sample splits.</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffffff0000">• However, it might have the same effect as pruning, so it might not be so bad for the algorithm actually</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffffffffff">• testing now the accuracy implications in python!!!</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">  ◇ </rich_text>
			<rich_text foreground="#ffff00000000">noifbinary</rich_text>
			<rich_text foreground="#ffffa5a50000"> has negligible effect for abalone</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">

print trees (cpp and python) for easy validation</rich_text>
			<rich_text>
   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">took the EasyXGB tree.cpp one
   •  skip python for now</rich_text>
			<rich_text>

-------------
predict

so far the values seem correct, but L/R is sometimes opposite to python (on 2-&gt;1-1 split, the wrong sample is in the wrong leaf?)
   • </rich_text>
			<rich_text foreground="#ffffa5a50000"> it was again due to the if-binary bullshit</rich_text>
			<rich_text>

theos code also passes the “leftover” samples to predict. (After giving each tree e.g. 66). But it should not matter in theory.

</rich_text>
			<rich_text foreground="#9090eeee9090">somehow the leaf_clipping = True does not make it to the point where it's used.
</rich_text>
			<rich_text>   •  </rich_text>
			<rich_text foreground="#ffffa5a50000">it's not passed when the actual tree is built. great.</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#ffffffff0000">• let's hope it was not a “convenient accident”</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffff00000000">◇ </rich_text>
			<rich_text>python bench: definitely noticeable for very small privacy budgets
         ▪ (like 130 instead of 120 for pb=0.1)
         </rich_text>
			<rich_text foreground="#ffff00000000">▪ TODO doing it again for verification</rich_text>
			<rich_text>
            </rich_text>
			<rich_text foreground="#ffffa5a50000">- 2nd run showed no difference -&gt; small to no effect</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffff00000000">◇ let's see effect on 2nd split</rich_text>
			<rich_text>
         ▪ </rich_text>
			<rich_text foreground="#ffffffff0000">DFS got better, 2-nodes got worse. Weird.</rich_text>
			<rich_text>
            - doing it again for verification.
            </rich_text>
			<rich_text foreground="#ffffa5a50000">- 2nd try looks almost identical. So kinda false alarm</rich_text>
			<rich_text>
     </rich_text>
			<rich_text foreground="#ffff00000000"> ◇ is it really required by the proof?</rich_text>
			<rich_text>
   • at least the gradient_filtering is used by the code.



</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">TODO need to read the results part of the base paper, to better understand the RMSE numbers and leaf clipping stuff. And the effect of having multiple ensembles</rich_text>
			<rich_text>

</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">and put into a presentation!
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">does DPBoost (base paper) have a github?
    </rich_text>
			<rich_text foreground="#ffffa5a50000">yup </rich_text>
			<rich_text foreground="#ffffa5a50000" link="webs https://github.com/QinbinLi/DPBoost/">https://github.com/QinbinLi/DPBoost/</rich_text>
			<rich_text foreground="#ffffa5a50000"> 
    but code example broken and in general it's messy
</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#9090eeee9090">    
why the fuck are the base paper absolute values different from theos?
</rich_text>
			<rich_text>   •  is it because they use budget like 0.5 as smallest and theo 0.1?
        is it because theo only has 1 ensemble?? (&quot;Boosting effect?&quot;)
        I mean there's some code there for multiple ensembles. But Theo didn't use it so it probably doesn't work.
      ◇</rich_text>
			<rich_text foreground="#ffffa5a50000"> nope, there are DPBoost 1 ensemble graphs.</rich_text>
			<rich_text>


Tomorow
</rich_text>
			<rich_text foreground="#9090eeee9090">   ◇ read DPBoost results etc carefully
   ◇ read theos results and compare.</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffffa5a50000">▪ Theos is worse, why though</rich_text>
			<rich_text>
   </rich_text>
			<rich_text foreground="#9090eeee9090">◇ start the presentation</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#9090eeee9090">      ▪ does he get the results he's supposed to?
</rich_text>
			<rich_text>         </rich_text>
			<rich_text foreground="#ffffa5a50000">◇   kinda close, but not quite (0-2x less accurate)</rich_text>
			<rich_text>
</rich_text>
			<rich_text foreground="#9090eeee9090">      ▪ 1 slide with everyhing that I don't agree with in theos code
         - how does this affect the results
      ▪ 1 slide about how I don't trust the parameters
</rich_text>
			<rich_text>         - need to settle for some, to have a benchmark basis &amp; start side-channeling
   </rich_text>
			<rich_text foreground="#9090eeee9090">◇ browse DPBoost github a bit.</rich_text>
			<rich_text>
      </rich_text>
			<rich_text foreground="#ffffa5a50000">▪ their abalone code example bugs at the moment :(</rich_text>
			<rich_text>
   ◇ finish the code
      ▪ add to presentation how many lines it is
         - 2-nodes shouldn't be much more, as it only affects splitting &amp; pb
         - however the accuracy benefits are small (worth for small datasets though?)
      ▪ and include some latex functions.
   ◇ other datasets :O !

============================

had differences with more samples being use for prediction in theo code
   • made them both start with 3300 and ignore some samples
      ◇ floor( train set / 50 )

STILL SMALL ERROR SOMEWHERE, INVESTIGATE LIKE TODAY. then create primitive validation infrastructure, then go on with todo list

    from there on
                    1103c1104
            &lt; :  Attr3 &lt; 0.225
            ---
            &gt; :  Attr5 &lt; 0.753
            
            it was because cpp gradient filtering was disabled. python does [-1,1]
            
now the tree diffs are identical . (except roundiing)

predictions seem to match, (just by checking some by hand)

============================

now figure out the scores.

so the predictions from both py and cpp have the same values, and their sum is equal (-0.841) so why the fuck does the py code go to -8?
   •  first forgot the init_value
   •  because of the inverse MinMaxScaler still wrong
      ◇ where is that -1.25 from
      ◇ ok, took inverse scaliing code from MinMaxScaler

laplace distribution sampling seems fine (tested with python plot)


    SO WHY THE HELL DO WE HAVE MORE FLUCTUATION





</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="questions 2.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1622471704.32" ts_lastsave="1624271025.05" unique_id="12">
			<rich_text foreground="#ffffc0c0cbcb">TODO: debug the result of the pick-formula (algo 2, line 8, base paper)
    does it stay constant across trees?
    is there an easier formula that calculates the same if we have only 1 ensemble ?</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#9090eeee9090">do we even need a test set for the “real” hardened application?</rich_text>
			<rich_text>
    
</rich_text>
			<rich_text foreground="#ffffc0c0cbcb">TODO read check out theos meeting slides

TODO   </rich_text>
			<rich_text>is use_decay  used?

</rich_text>
			<rich_text foreground="#9090eeee9090">Only y is scaled right? because we only will add noise to predictions. Right? Or do we need to scale X before training? Or does it not make a difference?</rich_text>
			<rich_text>
    </rich_text>
			<rich_text foreground="#ffffa5a50000">Esfandiar says only the labels (y)</rich_text>
			<rich_text>

Do we need to look at RMSE % compared to output range? 
    
</rich_text>
			<rich_text foreground="#ffffffff0000">I don't understand why scaling y in example.py gets such a low RMSE compared to cross_val_score</rich_text>
			<rich_text>
        it has the same contents as y in cross_val (and cpp)
        idk.
        ignore for now.
</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="questions 3.0" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624106103.59" ts_lastsave="1624106407.52" unique_id="16">
			<rich_text foreground="#9090eeee9090">After changing the algorithm, the parameters (grid search) are probably off. How to find them again effitiently?</rich_text>
			<rich_text>
        implement whole gridsearch stuff?</rich_text>
		</node>
		<node custom_icon_id="0" foreground="#a020f0" is_bold="False" name="&quot;mistakes&quot;" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624367636.05" ts_lastsave="1624438300.77" unique_id="17">
			<rich_text>leaving away leaf_clipping.
   •  On tree creation it is ready to receive 17 parameters.
      ◇ But theo supplies 16, exactly w/o leaf clipping so the default = False kicks in.</rich_text>
		</node>
		<node custom_icon_id="0" foreground="" is_bold="False" name="DPBoost" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624435164.19" ts_lastsave="1624435968.45" unique_id="18">
			<rich_text>• GDF suffices for the delta_g and delta_v formulas that we use
   ◇ So leaf_clipping seems optional?
   ◇ And the way theo implemented GDF seems wrong??
      ▪ GDF provides same sensitivities across all trees, unlike GLC.

• GLC's sensitivity bound gets tighter with the iterations

• Then the leaf node sensitivity bounds can be combined when using both
   ◇ delta_v = min(sens GDF, sens GLC)
   ◇ sensitivity (&quot;range of function&quot;) determines how much noise has to be added for DP</rich_text>
		</node>
	</node>
	<node custom_icon_id="0" foreground="#0000ff" is_bold="False" name="Further TODOs" prog_lang="custom-colors" readonly="False" tags="" ts_creation="1624438358.97" ts_lastsave="1624533654.25" unique_id="19">
		<rich_text scale="h2">Algorithm</rich_text>
		<rich_text>

• need to fix the sample choice to use the paper formula

• my </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">GDF</rich_text>
		<rich_text> is like theo's, but is that what DPBoost / paper says?

•  right now i'm splitting with “&lt;” like theo. 
   ◇ in _predict as well as in </rich_text>
		<rich_text foreground="#268bd2">samples_left_right_partition</rich_text>
		<rich_text>
   ◇ easyXGB does &lt;=. however it does not affect the gain. But still unsure what's right.

• my </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">2-sample-gain-split</rich_text>
		<rich_text> 2 -&gt; (1,1)
   ◇ compare that to DPBoost?
   ◇ Or does it not really matter for larger datasets? Hold for now.

• </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">Use_decay</rich_text>
		<rich_text>. (pb for internal nodes, “because upper splits are more important”) according to Theo thesis it can be nice, but for deep trees it gets bad. Need to check for which datasets it makes sense.

• </rich_text>
		<rich_text foreground="#ffffc0c0cbcb">2-nodes</rich_text>
		<rich_text>: is it worth it? apparently it should be good for small amount of samples (according to theo meeing notes)

</rich_text>
		<rich_text scale="h2">CPP Bonus</rich_text>
		<rich_text>
• read style guide

• stackoverflow code review?


</rich_text>
		<rich_text scale="h2">Bonus*</rich_text>
		<rich_text>

• Some docs on the cpp algorithm
   ◇ like intuition which parameters are suitable for which use case</rich_text>
	</node>
</cherrytree>
