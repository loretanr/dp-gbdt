\chapter{Related Work}\label{chap:related_work}

In this chapter, we give a brief overview of other privacy preserving work, that use different mechanisms than differential privacy. In particular, we focus on membership privacy.

Nasr et al. (2018) \cite{mia_reg} opt to turn the problem into a \textit{min-max privacy game}, and design a training algorithm that both minimises the prediction loss of the model as well as the maximum gain of the best membership inference attack. By considering the strongest membership inference attack available to an attacker, the defender can ensure that his model will provide the best protection. Their method is designed to work on classification models. They showed that on popular classification dataset Purchase-100, their method only decreased baseline accuracy by 3.6\%, while reducing the membership attack success rate from 67.6\% to 51.6\%.

In \cite{mia_memguard}, Jia et al. (2019) introduced MemGuard. MemGuard offers defences against membership privacy attacks while providing utility-loss guarantees. As opposed to tampering with the training process of the algorithm such as in differential privacy, the authors propose to carefully add noise to the vector of confidence score. This vector is equivalent to the prediction vector with probabilities that is depicted in Figure ~\ref{fig:shadow}. The amount of noise that is added to the prediction vector is not enough to change the predicted label itself, but is enough to trick the adversary's attacking model. 

In their design of a membership inference attack, Shokri et al. (2017) \cite{shokri} also give pointers to mitigation strategies:
\begin{enumerate}
	\item \textbf{Restrict the prediction vector to the top $k$ classes only}: the more classes there are in a dataset, the more information the model leaks. If there are many classes that are present in small quantities only, restricting the model to the top $k$ classes will still result in useful outputs. Having a small $k$ will reduce the attack success.
	\item \textbf{Coarsen precision of the prediction vector}: this is similar to \cite{mia_memguard}, but instead of adding noise to the prediction vector, its probabilities are rounded to $d$ digits. The smaller $d$ is, the less information the model leaks.
	\item \textbf{Use regularization}: since models that overfit too much the data will tend to be more vulnerable to membership inference attacks, regularisation techniques such as the $L_2$-norm standard regularisation can be used to counter overfitting.
\end{enumerate}

As shown in above research, differential privacy is not the one solution to many problems, but rather it is a potential solution to some specific problems, and sometimes differential privacy is not the way to go. Membership privacy (or more generally privacy preserving machine learning) is an active area of research, and there's still room for improvement and for finding a one-fit-all solution. 