%% LyX 2.3.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[a4paper,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\special{papersize=\the\paperwidth,\the\paperheight}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%% use full paper
\usepackage[margin=2cm]{geometry}

%% multicols
\usepackage{multicol}

%% no automated date after title 
\date{}

%% compact spacing
\usepackage[compact]{titlesec}
\usepackage{enumitem}
\setlist{nolistsep}

%% table formatting
\usepackage{graphicx} 

%% colors
\usepackage{xcolor, amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\DontPrintSemicolon

% Define pseudocode formatting
\SetAlgoVlined
\renewcommand{\KwSty}[1]{\textnormal{\textcolor{amaranth!90!black}{\bfseries #1}}\unskip}
\renewcommand{\ArgSty}[1]{\textnormal{\ttfamily #1}\unskip}
\SetKwComment{Comment}{\color{green!80!black!190}// }{}
\renewcommand{\CommentSty}[1]{\textnormal{\ttfamily\color{green!80!black!190}#1}\unskip}
\newcommand{\assign}{\leftarrow}
\newcommand{\var}{\texttt}
\newcommand{\FuncCall}[2]{\texttt{\bfseries #1(#2)}}
\SetKwProg{Function}{function}{}{}
\SetKw{Continue}{continue}
\SetKw{KwTo}{to}
\SetKw{KwInn}{in}
\SetKw{KwAnd}{and}
\SetKw{KwOr}{or}
\SetKw{KwForall}{forall}
\renewcommand{\ProgSty}[1]{\texttt{\bfseries #1}}
\SetAlFnt{\small}
\usepackage{setspace}
%\AtBeginEnvironment{algorithm}{\setstretch{0.75}}

% Pseudocode title
\SetAlgorithmName{Pseudocode}{algorithmautorefname}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\floatname{algorithm}{Pseudocode}

% text highlight
\newcommand{\hyellow}[1]{\colorbox{yellow!20}{#1}}
\newcommand{\horange}[1]{\colorbox{orange!20}{#1}}
\newcommand{\hgreen}[1]{\colorbox{green!20}{#1}}
\newcommand{\hred}[1]{\colorbox{red!30}{#1}}

% titlespace
\usepackage{titling}

% no ugly indent
\setlength\parindent{0pt}

% algorithm highlight
\usepackage{tikz}
\usetikzlibrary{fit,calc}
\newcommand{\boxi}[2]{
	\tikz[remember picture,overlay] \node (A) {};\ignorespaces
	\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(0,0.15\baselineskip)$)($(A)+(.946\linewidth,-{#2}\baselineskip - 0.25\baselineskip)$)}] {};}\ignorespaces
}
\newcommand{\boxit}[2]{
	\tikz[remember picture,overlay] \node (A) {};\ignorespaces
	\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(0,0.15\baselineskip)$)($(A)+(.915\linewidth,-{#2}\baselineskip - 0.25\baselineskip)$)}] {};}\ignorespaces
}
\newcommand{\boxitt}[2]{
	\tikz[remember picture,overlay] \node (A) {};\ignorespaces
	\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(0,0.15\baselineskip)$)($(A)+(.876\linewidth,-{#2}\baselineskip - 0.25\baselineskip)$)}] {};}\ignorespaces
}
\newcommand{\boxittt}[2]{
	\tikz[remember picture,overlay] \node (A) {};\ignorespaces
	\tikz[remember picture,overlay]{\node[yshift=3pt,fill=#1,opacity=.25,fit={($(A)+(0,0.15\baselineskip)$)($(A)+(.837\linewidth,-{#2}\baselineskip - 0.25\baselineskip)$)}] {};}\ignorespaces
}

\colorlet{green}{green!30}
\colorlet{yellow}{yellow!40}
\colorlet{red}{red!30}

% algorithm comments on the right side
\usepackage{algpseudocode, float}
%\renewcommand{\algorithmiccomment}[1]{\bgroup\hfill\tiny//~#1\egroup}

\makeatother

\usepackage{babel}
\usepackage{listings}
\lstset{frame=trbl,
backgroundcolor={\color{lightgray}},
flexiblecolumns=true,
basicstyle={\small\ttfamily},
breaklines=true,
keywordstyle={\color{blue}\bfseries},
language=Java,
sensitive=true,
emph={[1]{critical_section}},
emphstyle={[1]\color{red}},
emph={[2]{atomic,Condition}},
emphstyle={[2]\color{blue}},
rulesepcolor={\color{gray}},
emph={[3]{acquire(mutex),release(mutex),signal(mutex)}},
emphstyle={[3]\color{magenta}},
showstringspaces=false,
stringstyle={\color{purple}},
commentstyle={\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small},
morecomment={[l][\color{Blue}]{...}},
tabsize=4,
lineskip={-1.5pt}}
\begin{document}
\title{C\texttt{++} DP-GBDT Side-channel Analysis}

\maketitle
\tableofcontents{}

\paragraph{\hred{TODO}}
\begin{itemize}
\item clean up color usage (probably use yellow to mark my ``unnecessary
hardening'')
\item clean up right hand side comments
\item check function definitions, calls, arguments etc to be matching with
each other
\end{itemize}
\pagebreak{}

\section{Secrecy}

\subsection{Dataset and parameters}

\begin{tabular}{|l|c|}
\hline 
entity & secret\tabularnewline
\hline 
\hline 
content of X & $\checkmark$\tabularnewline
\hline 
X\_cols\_size & $\times$\tabularnewline
\hline 
X\_rows\_size & $\times$\tabularnewline
\hline 
content of y & $\checkmark$\tabularnewline
\hline 
y\_rows\_size & $\times$\tabularnewline
\hline 
\end{tabular} ~~~%
\begin{tabular}{|l|c|}
\hline 
parameter & secret\tabularnewline
\hline 
\hline 
nb\_trees & $\times$\tabularnewline
\hline 
learning\_rate & $\times$\tabularnewline
\hline 
privacy\_budget & $\times$\tabularnewline
\hline 
task & $\times$\tabularnewline
\hline 
max\_depth & $\times$\tabularnewline
\hline 
min\_samples\_split & $\times$\tabularnewline
\hline 
balance\_partition & $\times$\tabularnewline
\hline 
gradient\_filtering & $\times$\tabularnewline
\hline 
leaf\_clipping & $\times$\tabularnewline
\hline 
scale\_y & $\times$\tabularnewline
\hline 
use\_decay & $\times$\tabularnewline
\hline 
l2\_threshold & $\times$\tabularnewline
\hline 
l2\_lambda & $\times$\tabularnewline
\hline 
cat\_idx & $\times$\tabularnewline
\hline 
num\_idx & $\times$\tabularnewline
\hline 
\end{tabular} ~~~%
\begin{tabular}{|l|c|}
\hline 
inferrable from those & secret\tabularnewline
\hline 
\hline 
nb\_samples per tree & $\times$\tabularnewline
\hline 
\end{tabular}

\paragraph{While building a single tree\protect \\
}

\begin{tabular}{|c|c|c|}
\hline 
entity & secret & \tabularnewline
\hline 
\hline 
X\_subset & $\checkmark$ & \tabularnewline
\hline 
X\_subset\_cols\_size & $\times$ & \tabularnewline
\hline 
X\_subset\_rows\_size & $\checkmark$ & \tabularnewline
\hline 
y\_subset & $\checkmark$ & \tabularnewline
\hline 
y\_subset\_rows\_size & $\checkmark$ & \tabularnewline
\hline 
gradients & $\checkmark$ & \tabularnewline
\hline 
gradients\_size & $\checkmark$ & \tabularnewline
\hline 
\end{tabular}

\bigskip{}


\subsection{DP imperfections of the algorithm}
\begin{itemize}
\item \texttt{init\_score}, (=mean in regression, =most common feature in
classification) leaks information about which feature values are in
the dataset. Would need to add noise.
\item \texttt{compute\_gain} is done on the real data points. This was not
adressed by the DPBoost paper. Would also need to add noise there.
\item GDF is also problematic. changing one data point could have an impact
on 2 trees.
\end{itemize}
\pagebreak{}

\section{\texttt{main}}

No data dependent operations are done, therefore no side channel leakage.
Securely getting the model parameters and dataset into and the resulting
model out of the enclave is not among the challenges of this thesis.

\begin{algorithm}\setstretch{0.9}
  \caption{main}
  \Function {main()}{
	
	\tcp{get parameters and dataset}
	$\var{parameters} = \texttt{get\_params()}$\;
	$\var{dataset} = \texttt{get\_dataset()}$\;
	
	\tcp{create 5 train/test splits for cross validation}
	$\var{cv\_splits} = \texttt{create\_cross\_val\_inputs(dataset, 5)}$\;
	
	\tcp{do cross validation}
	\For {split \KwInn cv\_splits} {
		$\var{ensemble} = \texttt{DPEnsemble(parameters)}$\;
		$\emph{\var{ensemble.train(split.train)}}$\;

		\tcp{predict using the test set}
		$\var{y\_pred} = \emph{\var{ensemble.predict\_ensemble(split.test.X)}}$\;

		\tcp{compute score}
		$\var{score} = \var{compute\_score(split.test.y, y\_pred)}$\;
	}
  }
\end{algorithm}

\newpage{}

\section{\texttt{class DPTree}}

\subsection{Methods}

\subsubsection{\texttt{fit \hgreen{constant\_timed}}}

\paragraph{Caller/call graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/fit/class_d_p_tree_a82b9507f4f5d056b98f40871b304f943_icgraph}

\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/fit/class_d_p_tree_a82b9507f4f5d056b98f40871b304f943_cgraph}

\paragraph{Variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{dataset, leaves}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{params.{*}}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::fit}
  \Function {fit()}{
	
	\tcp{all samples are live at the start}
	$\var{live\_samples} = \texttt{[1,2,3,...,dataset.length]}$\;
	\tcp{build tree}
	$\var{this\ensuremath{\rightarrow}root\_node} = \emph{\texttt{make\_tree\_dfs(live\_samples, 0)}}$\;

	\tcp{leaf\_clipping} \boxit{green}{3.7}
	\If (\Comment{params}) {params.leaf\_clipping \KwOr !params.gradient\_filtering} {
		$\var{threshold} = \texttt{params.l2}\ensuremath{*(1-\eta)^{\texttt{tree\_index}}}$\;
		\boxitt{red}{1.2}
		\For(\Comment{number of leaves, which nodes are leaves}){leaf \KwInn this\ensuremath{\rightarrow}leaves} {
			$\var{leaf.prediction} = \texttt{clamp(leaf.prediction, -threshold, threshold)}$\;
		}

	}

	\tcp{add laplace noise to leaf values}
	$\var{privacy\_budget\_for\_leaf\_nodes} = \frac{\texttt{tree\_privacy\_budget}} {2}$\;
	$\var{laplace\_scale} = \frac{\var{params.\ensuremath{\Delta}v}}{\texttt{privacy\_budget\_for\_leaf\_nodes}}$\;
	$\emph{\var{add\_laplacian\_noise(laplace\_scale)}}$\;
	
  }
\end{algorithm}

\pagebreak{}

\subsubsection{\texttt{make\_tree\_dfs\hgreen{constant\_timed}}}

\paragraph{Caller/call graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/make_tree_DFS/class_d_p_tree_a07314c44bb7b7ca37c735b70de3ccb9e_icgraph}

\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/make_tree_DFS/class_d_p_tree_a54cea43a3929249de4be25f607a16339_cgraph}

\paragraph{Arguments / variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{dataset, X\_transposed, live\_samples, gradients}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{params.min\_samples\_split, params.max\_depth, curr\_depth}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::make\_tree\_dfs}
	
  \boxi{red}{0}
  \Function (\Comment{size of \texttt{live\_samples}}) {make\_tree\_dfs(live\_samples, curr\_depth)}{

	\tcp{max depth reached or not enough samples -> leaf node} \boxit{red}{2}
    \If{curr\_depth == params.max\_depth \KwOr len(live\_samples) < params.min\_samples\_split}{
			$\texttt{TreeNode *leaf = \emph{make\_leaf\_node(curr\_depth, live\_samples)}}$ \Comment{both branch conditions}
			
			\Return{$\texttt{leaf}$}\
    }
	
	\tcp{find best split}
	$\texttt{TreeNode *node = \emph{find\_best\_split(X, gradients, live\_samples, curr\_depth)}}$\;
		
	\tcp{no split found} \boxit{red}{1}
    \If (\Comment{number of leaves}) {node.is\_leaf }{
		\Return{$\texttt{node}$}\
    }

	\tcp{prepare the new live samples to continue recursion} \boxit{yellow}{0}
	$\var{lhs,rhs} = \texttt{\emph{samples\_left\_right\_partition(X, node.feature\_index, node.feature\_value)}}$ \Comment{sizes}

	\boxit{red}{4.5}
	\For (\Comment{size of \texttt{live\_samples}}) {sample \KwInn live\_samples} {
		\boxitt{red}{3}
		\eIf (\Comment{which samples go left/right}) {lhs.contains(sample)} {
			$\texttt{lhs\_live\_samples.insert(sample)}$\;
		} {
			$\texttt{rhs\_live\_samples.insert(sample)}$\;
		}
	}
	\tcp{recurse}
	$\texttt{node\ensuremath{\rightarrow}left = make\_tree\_dfs(lhs\_live\_samples, curr\_depth+1)}$\;
	$\texttt{node\ensuremath{\rightarrow}right = make\_tree\_dfs(rhs\_live\_samples, curr\_depth+1)}$\;
    \Return{$\texttt{node}$}\
  }
\end{algorithm}

\paragraph{Recursion leakage}
\begin{itemize}
\item \hred{number of splits in the tree}
\item \hred{number of splits/leaves observable by watching memory allocations}
\end{itemize}
\newpage{}

\subsubsection{\texttt{exponential\_mechanism}}

\paragraph{Caller graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/exponential_mechanism/class_d_p_tree_a7ce368c3888736d660dcf09587df4512_icgraph}

\paragraph{Arguments / variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{candidates}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{-}

\begin{algorithm}\setstretch{0.9}
	\caption{DPTree::exponential\_mechanism}
	\boxi{red}{0}
	\Function (\Comment{number of candidates}) {exponential\_mechanism(candidates)}{

	\tcp{if no split with positive gain, return, node will become a leaf}
	\boxit{red}{1.2}
    \If (\Comment{no good split exists, leaf creation}){cand.gain <= 0 \KwForall cand \KwInn candidates} {  
		$\Return \var{-1}$\;
    }

	\tcp{calculate probabilities from the gains}
	\boxit{red}{6.8}
	\For(\Comment{number of candidates}){candidate \KwInn candidates} {
		$\var{gains.append(candidate.gain)}$\;
		\boxitt{red}{4.5}
		\eIf (\Comment{number of candidates with viable splits}) {candidate.gain <= 0} {
			$\var{probabilities.append(0)}$\;
		} {
			$\var{lse} = \ensuremath{\log\sum_{i}\exp(\texttt{gains}_{i})}$\;
			$\var{probabilities.append(\ensuremath{\exp ({\texttt{candidate.gain} - \var{lse}})})}$\;
		}
	}

	\tcp{create a cumulative distribution from the probabilities, its values add up to 1}
	$\var{partials} = \texttt{std::partial\_sum(probabilities)}$\;

	\tcp{choose random value in [0,1]}
	$\var{rand\_val} = \texttt{std::rand()}$\;

	\tcp{return the corresponding split}
	\For {i = 0 \KwTo i = partials.size() - 1} {
		\If {partials[i] >= rand\_val} {
			\Return $\var{i}$
		}
	}
	\Return $\var{-1}$

	
  }
\end{algorithm}

\newpage{}

\subsubsection{\texttt{find\_best\_split}}

\paragraph{Caller graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/find_best_split/class_d_p_tree_ad7f8755be37ad10cf104fba2076dce5e_icgraph}

\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/find_best_split/class_d_p_tree_ad7f8755be37ad10cf104fba2076dce5e_cgraph}

\paragraph{Arguments / variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{X, gradients, live\_samples}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{params.{*}, tree\_budget, curr\_depth}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::find\_best\_split}
	\boxi{red}{0}
  \Function (\Comment{size of \texttt{X, gradients}}) {find\_best\_split(X, gradients, live\_samples, curr\_depth)}{

	\tcp{determine node privacy budget}
	\boxit{green}{7.5}
    \eIf (\Comment{\texttt{params.use\_decay}}){params.use\_decay} {  
		\boxitt{green}{3.7}
		\eIf(\Comment{\texttt{curr\_depth == 0}}){curr\_depth == 0} {
			$\var{node\_budget} = \frac{\var{tree\_budget}}{2*({2^{\var{max\_depth}+1}} + 2^{\var{curr\_depth}+1})}$\;
	    } {
			$\var{node\_budget} = \frac{\var{tree\_budget}}{2*{2^{\var{curr\_depth}+1}}}$\;
		}
    } {
		$\var{node\_budget} = \frac{\var{tree\_budget}}{2*\var{max\_depth}}$\;
	}
	\tcp{iterate over all possible splits}
	\boxit{green}{9.8}
	\For(\Comment{number of cols in \texttt{X}}){feature\_index \KwInn features} {
		\boxitt{red}{8.5}
		\For(\Comment{number of rows in \texttt{X}}){feature\_value \KwInn X[feature\_index]} {
			\boxittt{red}{1}
			\If (\Comment{number of unique feature values}) {"already encountered feature\_value"} {
				\Continue
			}
			$\var{gain} = \texttt{\emph{compute\_gain(X, gradients, live\_samples, feature\_index, feature\_value)}}$\;
			\boxittt{red}{1}
			\If (\Comment{number of splits with no gain}){gain < 0} {
				\Continue
			}
			$\var{gain} = \frac{\var{node\_budget} * \var{gain}}{2 * \Delta g}$\;
			\boxittt{red}{0}
			$\var{candidates}\texttt{.insert(Candidate(feature\_index, feature\_value, gain))}$ \Comment{number of candidates}	
		}
	}
	\tcp{choose a split using the exponential mechanism}
	$\var{index} = \texttt{\emph{exponential\_mechanism(candidates)}}$\;
	\tcp{construct the node} \boxit{yellow}{0}
	$\texttt{TreeNode *node = new TreeNode(candidates[index])}$ \Comment{internal node vs. leaf}

    \Return{$\texttt{node}$}\;
  }
\end{algorithm}

\pagebreak{}

\subsubsection{\texttt{compute\_gain}}

\paragraph{Caller/call graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/compute_gain/class_d_p_tree_ad7f8755be37ad10cf104fba2076dce5e_icgraph}

\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/compute_gain/class_d_p_tree_ab9426a6ac2e5122e29ecbb6e9c4cd050_cgraph}

\paragraph{Arguments / variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{X, gradients, live\_samples}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{params.l2\_lambda, feature\_index, feature\_value}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::compute\_gain}
  \boxi{red}{0}
  \Function (\Comment{\texttt{X, gradients}}) {compute\_gain(X, gradients, feature\_index, feature\_value)}{
	
	\tcp{partition into lhs/rhs}
	\boxit{red}{2}
     $\var{lhs}, \var{rhs} = \texttt{\emph{samples\_left\_right\_partition(X, feature\_index, feature\_value)}}$ \Comment{lhs/rhs size}
		
	$\var{lhs\_size} = \texttt{lhs.size()}$\;
	$\var{rhs\_size} = \texttt{rhs.size()}$\;

	\tcp{return on useless split} \boxit{red}{1}
	\If (\Comment{useless split}) {lhs\_size == 0 \KwOr rhs\_size == 0} {
		\Return{$\texttt{-1}$}
	}
	\tcp{sums of lhs/rhs gains}  \boxit{red}{1}
	$\var{lhs\_gain} = \texttt{sum(gradients[lhs])}$ \Comment{memory access pattern of left/right gradients}
	$\var{rhs\_gain} = \texttt{sum(gradients[rhs])}$\;
	$\var{lhs\_gain} = \frac{\var{lhs\_gain}^{2}}{\var{lhs\_size} + \var{params.l2\_lambda}}$\;
	$\var{rhs\_gain} = \frac{\var{rhs\_gain}^{2}}{\var{rhs\_size} + \var{params.l2\_lambda}}$\;
	\boxit{yellow}{0}
	$\var{total\_gain} = \texttt{max(\var{lhs\_gain} + \var{rhs\_gain}, 0)}$ \Comment{\texttt{max} might leak whether \texttt{total\_gain < 0}}
    
	\Return{$\texttt{total\_gain}$}
  }
\end{algorithm}

\newpage{}

\subsubsection{\texttt{make\_leaf\_node}}

\paragraph{Caller graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/make_leaf_node/class_d_p_tree_a3a716a075edfae470e0fe7511634e922_icgraph}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::make\_leaf\_node}
	\boxi{red}{0}
  \Function (\Comment{\var{live\_samples} size}) {make\_leaf\_node(curr\_depth, live\_samples)}{

	$\var{leaf} = \texttt{new TreeNode(curr\_depth)}$\;
	
	\tcp{compute prediction} \boxit{red}{1.2}
	\For (\Comment{\var{live\_samples} size}) {sample\_index \KwInn live\_samples} {
		$\var{gradients.append(dataset.gradients[sample\_index])}$ \Comment{\var{dataset.gradients} memory access pattern}
	}
	$\var{leaf.prediction} = -\frac{\sum \var{gradients}}{\var{gradients.size()} + \var{params.l2\_lambda}}$\;
	\Return \var{leaf}

  }
\end{algorithm}

\subsubsection{\texttt{samples\_left\_right\_partition}}

\paragraph{Caller graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/samples_left_right_partition/class_d_p_tree_af16f3e0373e606fe42b2104890b2d036_icgraph}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::samples\_left\_right\_partition}
	\boxi{red}{0}
  \Function (\Comment{\var{X} size}) {samples\_left\_right\_partition(X, feature\_index, feature\_value)}{

	\boxit{yellow}{15}
	\eIf (\Comment {cat/num feature split}) {feature\_index \KwInn params.cat\_idx} {

		\tcp{categorical feature} \boxitt{red}{4.7}
		\For (\Comment{\var{X} size}) {sample\_index = 0 \KwTo sample\_index < X.num\_rows} {
			\boxittt{red}{3}
			\eIf (\Comment{\var{lhs,rhs} sizes and access pattern}) {X[sample\_index][feature\_index] == feature\_value} {
				$\var{lhs.append(sample\_index)}$\;
			} {	
				$\var{rhs.append(sample\_index)}$\;
			}
		}
	} {

		\tcp{numerical feature} \boxitt{red}{4.7}
		\For (\Comment{\var{X} size}) {sample \KwInn X[feature\_index]} {
			\boxittt{red}{3}
			\eIf (\Comment{\var{lhs,rhs} sizes and access pattern}) {X[sample\_index][feature\_index] < feature\_value} {
				$\var{lhs.append(sample\_index)}$\;
			} {	
				$\var{rhs.append(sample\_index)}$\;
			}
		}
	}
	
	\Return \var{lhs,rhs}

  }
\end{algorithm}

Whether the yellow box is secret depends on the context from where
the function is called. If it's from find\_best\_split / compute\_gain
where we're just trying out all possible splits, then it's not secret.
However if we recreate the split that we found and chosen before,
then it's secret.

\subsubsection{\texttt{predict\_tree}}

\paragraph{Caller/call graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/predict_tree/class_d_p_tree_a1e28d69924f6d57ea7e57ad8ee138f42_icgraph}

\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/predict_tree/class_d_p_tree_a1e28d69924f6d57ea7e57ad8ee138f42_cgraph}

\begin{algorithm}[]\setstretch{0.9}
  \caption{DPTree::predict\_tree}

	\Function (\Comment{\var{X} size}) {predict\_tree(X)}{
		\For {row \KwInn X} {
			$\var{predictions.append(\emph{\_predict(row, this\ensuremath{\rightarrow}root\_node)})}$\;
		}
		\Return \var{predictions}
	}
	\;
	
	\boxi{red}{13}
	\Function (\Comment{must not leak whether a sample row goes left/right}) {\_predict(row, node)}{

		\If {node.is\_leaf} {
			\Return \var{node.prediction}
		}
		
		\eIf {node.split\_attribute \KwInn params.cat\_idx} {
			\tcp{categorical feature}
			\If {row[split\_attribute] == node.split\_value} {
				\Return \var{\emph{\_predict(row, node.left\_child)}}
			}
		} {
			\tcp{numerical feature}
			\If {row[split\_attribute] < node.split\_value} {
				\Return \var{\emph{\_predict(row, node.left\_child)}}
			}
		}
		\Return \var{\emph{\_predict(row, node.right\_child)}}

  }
\end{algorithm}

\subsubsection{\texttt{add\_laplacian\_noise}}

\paragraph{Caller/call graph}

~\\
\includegraphics[scale=0.55]{/home/loretanr/ma/code/hardening/visualisation/graphs/add_laplacian_noise/class_d_p_tree_a468d249a3e55f1527e50daf13e550c79_icgraph}

\begin{algorithm}\setstretch{0.9}
  \caption{DPTree::add\_laplacian\_noise}

  \Function {add\_laplacian\_noise(laplace\_scale)}{

	$\var{lap} = \texttt{new LaplaceDistribution(laplace\_scale)}$\;
	
	\tcp{add noise from laplace distribution to leaves} \boxit{yellow}{2}
	\For (\Comment{number of leaves}) {leaf \KwInn this.leaves} {
		$\var{noise = lap.return\_a\_random\_variable()}$\;
		$\var{leaf.prediction += noise}$\;
	}
  }
\end{algorithm}

\newpage{}

\section{\texttt{class DPEnsemble}}

TODO can I get around doing it for DPEnsemble? because it should be
largely data independent.
\begin{itemize}
\item I think it would be consequent to hide the GDF stuff as well, since
it gradients kinda depend on data. However, possible that it is also
``unnecessary'' and already assumed leaking in the proof.
\end{itemize}

\subsection{Methods}

\subsubsection{\texttt{train}}

\paragraph{Caller graph\protect \\
}

TODO

\paragraph{Call graph\protect \\
}

TODO

\paragraph{Variables}
\begin{itemize}
\item must not leak:
\end{itemize}
\texttt{TODO}
\begin{itemize}
\item can leak:
\end{itemize}
\texttt{params.{*}, tree\_params.{*}}

\begin{algorithm}\setstretch{0.9}
  \caption{DPEnsemble::train}
  \Function {train(dataset)}{
	
	\tcp{compute initial prediction}
	$\var{init\_score} = \emph{\var{compute\_init\_score(dataset.y)}}$\;

	\tcp{each tree gets the full budget since they train on distinct data}
	$\var{tree\_privacy\_budget} = \texttt{params.privacy\_budget}$\;

	\tcp{train all trees}
	\For (\Comment{bla}) {tree\_index = 0 \KwTo tree\_index = nb\_trees - 1} {

		\tcp{init/update gradients}
		$\emph{\var{update\_gradients(dataset.gradients, tree\_index)}}$\;

		\tcp{sensitivity for internal nodes}
		$\var{tree\_params.\ensuremath{\Delta g}} = \ensuremath{3*(\texttt{params.l2\_threshold})^2}$\;

		\tcp{sensitivity for leaf nodes}
		\eIf {params.gradient\_filtering \KwOr  !params.leaf\_clipping} {
			$\var{tree\_params.\ensuremath{\Delta v}} = \frac{\texttt{params.l2\_threshold}}{1+\texttt{params.l2\_lambda}} $\;
		}{
			$\var{tree\_params.\ensuremath{\Delta v}} = \ensuremath{\min (\frac{\texttt{params.l2\_threshold}}{1 + \texttt{params.l2\_lambda}}, 2* \texttt{params.l2\_threshold} * \ensuremath{(1-\eta)^{\texttt{tree\_index}}})}$\;
		}
	
		\tcp{determine number of rows}
		\eIf {params.balance\_partition} {
			$\var{number\_of\_rows} = \frac{|D|}{\texttt{nb\_trees}- \texttt{tree\_index}} $\;
		} {
			$\var{number\_of\_rows} = \frac{|D| \eta (1- \eta)^{\texttt{tree\_index}} } { 1 - (1-\eta)^{\texttt{nb\_trees}} } $\;
		}

	}

	
  }
\end{algorithm}

\pagebreak{}

\subsubsection{\texttt{predict}}

\subsubsection{\texttt{update\_gradients}}

\subsubsection{\texttt{add\_laplacian\_noise}}

\subsubsection{\texttt{remove\_rows}}

\subsubsection{\texttt{get\_subset}}

\section{other classes}
\end{document}
